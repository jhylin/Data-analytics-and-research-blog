{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Random forest classifier\n",
        "subtitle: Series 2.2.1 - more on imbalanced dataset\n",
        "author: Jennifer HY Lin\n",
        "date: 2024-1-6\n",
        "draft: true\n",
        "categories:\n",
        "  - Machine learning projects\n",
        "  - Tree models\n",
        "  - Pandas\n",
        "  - Scikit-learn\n",
        "  - ChEMBL database\n",
        "  - Python\n",
        "format: html\n",
        "bibliography: references.bib\n",
        "---"
      ],
      "id": "1cce939c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### **Brief introduction**\n",
        "\n",
        "This post was really just an addition towards the last random forest (RF) post (machine learning (ML) series 2.2). It was mainly inspired by this paper [@esposito2021] from [rinikerlab](https://github.com/rinikerlab). It was also nice to complete the RF series by adding a RF classifier model since last post was only on a regressor. Also, knowing that imbalanced datasets were common in drug discovery projects, learning new strategies to deal with them was also extremely useful[^1], and while I was working on this post, I also came across a few other packages that I haven't used before, so I've included them all down below.\n",
        "\n",
        "[^1]: 1\n",
        "\n",
        "<br>\n",
        "\n",
        "##### **Overview on packages/scripts used**\n",
        "\n",
        "-   Data source using *chembl_downloader*\n",
        "-   Own little script of random_forest.py (avoid repeating code)\n",
        "-   SMILES checker from *scikit_mol*\n",
        "-   Dealing with imbalanced datasets in RF classifiers by using *ghostml*\n",
        "\n",
        "<br>\n",
        "\n",
        "##### **Importing libraries**\n"
      ],
      "id": "c97959bb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import chembl_downloader\n",
        "from chembl_downloader import latest, queries, query\n",
        "from rdkit.Chem import Descriptors\n",
        "import datamol as dm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import RocCurveDisplay, roc_curve\n",
        "from scikit_mol.utilities import CheckSmilesSanitazion\n",
        "import ghostml"
      ],
      "id": "2dc10c7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "##### **Data retrieval using chembl_downloader**\n",
        "\n",
        "chembl_downloader package was something I wanted to try a while back after knowing that it would thoroughly document the ChEMBL data source (I've tried manual download and chembl_webresource_client, and they were probably not the best strategies for data reproducibility). Its idea was to generate a data source that could be fully reproducible. It involved some SQL at the beginning to specify the exact type of data needed, so some SQL knowledge was required. The rest were pretty straightforward. Other uses for this package were elaborated much more nicely in its GitHub repository at https://github.com/cthoyt/chembl-downloader. Overall, I think it was very useful in a Jupyter notebook or equivalent environments to document the data used.\n",
        "\n",
        "Reference notebooks that used chembl_downloader (more provided in its repository):\n",
        "\n",
        "-   https://github.com/cthoyt/chembl-downloader/blob/main/notebooks/drug-indications.ipynb\n",
        "\n",
        "-   https://github.com/PatWalters/practical_cheminformatics_tutorials/blob/1f7c61f83eec81081ef2605ac70440bf1940d914/misc/working_with_ChEMBL_drug_data.ipynb#L80\n",
        "\n",
        "What I did was shown below.\n"
      ],
      "id": "7b0beb35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show the latest version of ChEMBL used\n",
        "latest_version = latest()\n",
        "print(f\"The latest ChEMBL version is: {latest_version}\")"
      ],
      "id": "84d6998f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Query chembl_downloader to show SQL required to extract ChEMBL data for a specific protein target\n",
        "# e.g. target_chembl_id for AChE: CHEMBL220\n",
        "queries.markdown(queries.get_target_sql(target_id=\"CHEMBL220\", target_type=\"SINGLE PROTEIN\"))"
      ],
      "id": "ec0ed812",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code below took several minutes to run - ?show time\n",
        "\n",
        "``` {{python}}\n",
        "# Following data download worked (4.41GB for SQL query below, might take several min)\n",
        "\n",
        "# Added MOLECULE_DICTIONARY.max_phase into sql to show max phases for compounds targeting AChE\n",
        "\n",
        "sql = \"\"\"\n",
        "SELECT\n",
        "    ASSAYS.chembl_id              AS assay_chembl_id,\n",
        "    TARGET_DICTIONARY.target_type,\n",
        "    TARGET_DICTIONARY.tax_id,\n",
        "    TARGET_DICTIONARY.chembl_id,\n",
        "    COMPOUND_STRUCTURES.canonical_smiles,\n",
        "    MOLECULE_DICTIONARY.chembl_id AS molecule_chembl_id,\n",
        "    MOLECULE_DICTIONARY.max_phase,\n",
        "    ACTIVITIES.standard_type,\n",
        "    ACTIVITIES.pchembl_value\n",
        "FROM TARGET_DICTIONARY\n",
        "     JOIN ASSAYS ON TARGET_DICTIONARY.tid == ASSAYS.tid\n",
        "     JOIN ACTIVITIES ON ASSAYS.assay_id == ACTIVITIES.assay_id\n",
        "     JOIN MOLECULE_DICTIONARY ON MOLECULE_DICTIONARY.molregno == ACTIVITIES.molregno\n",
        "     JOIN COMPOUND_STRUCTURES ON MOLECULE_DICTIONARY.molregno == COMPOUND_STRUCTURES.molregno\n",
        "WHERE TARGET_DICTIONARY.chembl_id = 'CHEMBL220'\n",
        "    AND ACTIVITIES.pchembl_value IS NOT NULL\n",
        "    AND TARGET_DICTIONARY.target_type = 'SINGLE PROTEIN'\n",
        "\"\"\"\n",
        "\n",
        "df = chembl_downloader.query(sql)\n",
        "```\n",
        "\n",
        "```{{python}}\n",
        "# Save df as .csv file\n",
        "df.to_csv(\"chembl_d_ache\", sep=\",\", index=False)\n",
        "```\n"
      ],
      "id": "7af5526a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load dataset from saved .csv file\n",
        "df_ache = pd.read_csv(\"chembl_d_ache\")\n",
        "print(df_ache.shape)\n",
        "df_ache.head()"
      ],
      "id": "025330ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "##### **Some data cleaning**\n",
        "\n",
        "Minor cleaning and preprocessing were done for this post only, as the focus was more on dealing with imbalanced datasets in RF classifier.\n",
        "\n",
        "<br>\n",
        "\n",
        "###### **mol_prep.py**\n",
        "\n",
        "I've written or more like compiled my own little pieces of code into a Python script. The idea was to remove most function code in the post to avoid repeating them as they've been used frequently in the last few posts. The script would be saved into my project repository, and it would still be a \"work-in-progress\" script as hopefully I'll work on it further in the future.\n"
      ],
      "id": "c45426f2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Trial own mol_prep.py script\n",
        "from mol_prep import preprocess, rdkit_2d_descriptors"
      ],
      "id": "6db6e5e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Preprocess/standardise molecules\n",
        "# Running preprocess function \n",
        "df_ache = df_ache.copy()\n",
        "df_prep = df_ache.apply(preprocess, axis = 1)\n",
        "df_prep.head(3)"
      ],
      "id": "3582f333",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "###### **scikit_mol**\n",
        "\n",
        "scikit_mol was a package originated from 2022 RDKit UGM hackathon. This [blog post](https://www.cheminformania.com/scikit-mol-easy-embedding-of-rdkit-into-scikit-learn/) elaborated further on its functions and uses in machine learning. For this post I've only used it for a very small portion, mainly to check for missing SMILES or errors in SMILES (kind of like double checking whether my preprocess function code worked as expected). It could be integrated with scikit-learn's pipeline method on multiple estimators. GitHub Repository link: <https://github.com/EBjerrum/scikit-mol>\n"
      ],
      "id": "1a5c9c03"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Quick simple way to check for missing SMILES\n",
        "print(f'Dataset contains {df_prep.standard_smiles.isna().sum()} unparsable mols')"
      ],
      "id": "3ce53358",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It appeared to have no unparsable (missing) molecules.\n"
      ],
      "id": "cbf0214c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Checking for invalid SMILES using scikit_mol\n",
        "smileschecker = CheckSmilesSanitazion()\n",
        "\n",
        "smileschecker.sanitize(list(df_prep.standard_smiles))\n",
        "\n",
        "# Showing SMILES errors\n",
        "smileschecker.errors"
      ],
      "id": "1117fd24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It also confirmed that there were no errors in SMILES.\n"
      ],
      "id": "089d3de2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Generate RDKit2D descriptors/fingerprints\n",
        "# Running rdkit_2d_descriptors function\n",
        "df_2d = rdkit_2d_descriptors(df_prep)\n",
        "df_2d.head(3)"
      ],
      "id": "83fa522b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Merge dataframes df_prep & df_2d via index\n",
        "df_merge = pd.merge(\n",
        "    df_prep[[\"max_phase\", \"molecule_chembl_id\"]],\n",
        "    df_2d,\n",
        "    left_index=True,\n",
        "    right_index=True\n",
        ")"
      ],
      "id": "06cb0e64",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(df_merge.shape)\n",
        "df_merge.head(3)"
      ],
      "id": "7b9e5d15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A different spreads of max phases were shown this time as the SQL query mainly used IC50, whereas last post was strictly limited to Ki via ChEMBL web resource client. Other likely reason was that in the decision tree series, I attempted data preprocessing at a larger scale (which eliminated some data). So, it appeared that there were more max phase 4 compounds here than last time (Note: null compounds were not shown in the value counts here as it was labelled as \"NaN\", but it should be the largest portion of max phase in the data).\n"
      ],
      "id": "e5ce8892"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find out counts of each max phase\n",
        "df_merge.value_counts(\"max_phase\")"
      ],
      "id": "76ce4370",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I then tried searching for the chembl_id of the 10 max phase 4 compounds used in the last post in df_merge.\n"
      ],
      "id": "81e4722e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Previously used 10 max phase 4 compounds\n",
        "# donepezil = CHEMBL502 \n",
        "# galantamine = CHEMBL659\n",
        "list_mp4 = [\"CHEMBL95\", \"CHEMBL1128\", \"CHEMBL640\", \"CHEMBL502\", \"CHEMBL481\", \"CHEMBL360055\", \"CHEMBL1025\", \"CHEMBL659\", \"CHEMBL1200970\", \"CHEMBL1677\"]\n",
        "\n",
        "# Search for compounds in list_mp4 within df_merge's \"molecule_chembl_id\" column\n",
        "# using Series.isin\n",
        "df_prev = df_merge.loc[df_merge[\"molecule_chembl_id\"].isin(list_mp4)]\n",
        "df_prev"
      ],
      "id": "94ea4c84",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There were many duplicates of compounds.\n"
      ],
      "id": "bafede15"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Some compounds with duplicates\n",
        "print(df_prev.shape)\n",
        "df_prev.value_counts(\"molecule_chembl_id\")"
      ],
      "id": "864abb0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Dropping duplicated compound via chembl IDs in the main df\n",
        "df_merge_new = df_merge.drop_duplicates(subset=[\"molecule_chembl_id\"], keep=\"first\")\n",
        "print(df_merge_new.shape)\n",
        "df_merge_new.head()"
      ],
      "id": "4b11c3a1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Previously used 10 max phase 4 compounds were found in df_merge_new\n",
        "df_mp4 = df_merge_new.loc[df_merge_new[\"molecule_chembl_id\"].isin(list_mp4)]\n",
        "df_mp4"
      ],
      "id": "38cbc65f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# note: compounds with max phase 0 not shown in the count\n",
        "df_merge_new.value_counts(\"max_phase\")"
      ],
      "id": "f33bf2ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "##### **Model building**\n",
        "\n",
        "The aim was to model and classify the max phase of ChEMBL small molecules - max phase 4 or not:\n",
        "\n",
        "    - *target*: max_phase\n",
        "    - *features*: various RDKit 2D descriptors (RDKit2D)\n",
        "\n",
        "1.  Re-labelled max phases as binary labels (e.g. max phase null as 0, max phase 4 as 1)\n"
      ],
      "id": "5a5202fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Re-label max phase NaN as 0\n",
        "df_merge_new = df_merge_new.fillna(0)\n",
        "df_merge_new"
      ],
      "id": "3a83e970",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  Splitting data into max phase null & max phase 4 (reason being needing to re-label max phase 4 column only as 1, and not disrupting max phase 0 compounds)\n"
      ],
      "id": "ed108726"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select all max phase null compounds\n",
        "df_null = df_merge_new[df_merge_new[\"max_phase\"] == 0]\n",
        "print(df_null.shape)\n",
        "df_null.head()"
      ],
      "id": "a165a8b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Using pd.DataFrame.assign to add a new column to re-label max_phase 4 into \"1\"\n",
        "df_mp4_lb = df_mp4.assign(max_phase_lb = df_mp4[\"max_phase\"] / 4)\n",
        "\n",
        "# Using pd.DataFrame.pop() & insert() to shift added column to first column position\n",
        "first_col = df_mp4_lb.pop(\"max_phase_lb\")\n",
        "df_mp4_lb.insert(0, \"max_phase_lb\", first_col)\n",
        "\n",
        "df_mp4_lb"
      ],
      "id": "7bb610b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Also create a new column max_phase_lb column for df_null to merge 2 dfs together\n",
        "\n",
        "df_null_lb = df_null.assign(max_phase_lb = df_null[\"max_phase\"])\n",
        "first_col_null = df_null_lb.pop(\"max_phase_lb\")\n",
        "df_null_lb.insert(0, \"max_phase_lb\", first_col_null)\n",
        "df_null_lb.head()"
      ],
      "id": "fee32fae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The two dataframes should share same column names which could be combined together.\n"
      ],
      "id": "c892af85"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Concatenate df_mp4_lb & df_null_lb\n",
        "df_full = pd.concat([df_null_lb, df_mp4_lb])\n",
        "df_full"
      ],
      "id": "a7db7995",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3.  Define X and y variables and train RF classifier model\n",
        "\n",
        "Eventually df_full contained 10 active compounds and 5256 inactive compounds, as shown from the value counts.\n"
      ],
      "id": "bf6e293d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_full.value_counts(\"max_phase_lb\")"
      ],
      "id": "86494e67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cast max_phase_lb column into integer\n",
        "df_full[\"max_phase_lb\"] = df_full[\"max_phase_lb\"].astype(int)"
      ],
      "id": "d458ae7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_full.head()"
      ],
      "id": "aca6d724",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Defining X (features) & y (target)\n",
        "X = df_full.iloc[:, 3:]\n",
        "y = df_full.iloc[:, 0]"
      ],
      "id": "b9095f95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Checking the right data have been selected\n",
        "y"
      ],
      "id": "00086a42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert both X & y to arrays\n",
        "X = X.to_numpy()\n",
        "y = y.to_numpy()"
      ],
      "id": "589c0b8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Using train_test_split() this time to split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)"
      ],
      "id": "ea885b85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Training RF classifier model\n",
        "\n",
        "Reference notebook: https://github.com/rinikerlab/GHOST/blob/main/notebooks/example_GHOST.ipynb\n"
      ],
      "id": "e88b2c1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Using RandomForestClassifier() to train model\n",
        "# max_features = \"sqrt\" by default, I decided to show it explicitly in the code to make sure the right one was used\n",
        "rfc = RandomForestClassifier(max_depth=3, random_state=1, max_features=\"sqrt\", oob_score=True)\n",
        "rfc.fit(X_train, y_train)"
      ],
      "id": "c64dbca9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4.  Obtain the prediction probabilities on the testing data and show confusion matrix with classification metrics\n"
      ],
      "id": "1ab307e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract positive prediction probabilities for the testing set\n",
        "test_probs = rfc.predict_proba(X_test)[:, 1]"
      ],
      "id": "643005dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A useful web link on area under the receiver operating characteristic curve - https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc\n"
      ],
      "id": "1298d83c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Rather than re-inventing the wheel, the following function code for calculating metrics of the RF model was adapted from Landrum et al., from this notebook: https://github.com/rinikerlab/GHOST/blob/main/notebooks/example_GHOST.ipynb\n",
        "\n",
        "def calc_metrics(y_test, test_probs, threshold = 0.5):\n",
        "    # Scores to differentiate between stated decision threshold (default = 0.5)\n",
        "    scores = [1 if x>=threshold else 0 for x in test_probs]\n",
        "    # Calculate area under the receiver operating characteristic curve\n",
        "    auc = metrics.roc_auc_score(y_test, test_probs)\n",
        "    # Calculate Cohen's Kappa score\n",
        "    kappa = metrics.cohen_kappa_score(y_test, scores)\n",
        "    # Formulate the confusion matrix\n",
        "    confusion = metrics.confusion_matrix(y_test, scores, labels = list(set(y_test)))\n",
        "    print('thresh: %.2f, kappa: %.3f, AUC test-set: %.3f'%(threshold, kappa, auc))\n",
        "    print(confusion)\n",
        "    print(metrics.classification_report(y_test,scores))\n",
        "    return "
      ],
      "id": "fddea2de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initially I made a mistake when doing the data split, which created a problem with y_test which only contained one label in it (1 only), needs binary labels (0 & 1)\n",
        "\n",
        "It was needed for roc_auc_score (measures true +ve & false +ve rates).\n"
      ],
      "id": "582e3665"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show confusion matrix along with AUC and Cohen's Kappa\n",
        "calc_metrics(y_test, test_probs, threshold = 0.5)\n",
        "\n",
        "# This was an extreme case - as only 10 actives vs. 5256 inactives"
      ],
      "id": "c9dadca1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5.  two approaches from GHOST paper (main idea was optimising and shifting decision threshold):\n",
        "\n",
        "    -   approach 1 (out-of-bag method, aimed for RF classifiers) based on RDKit blog post (ref. 41) - http://rdkit.blogspot.com/2018/11/working-with-unbalanced-data-part-i.html ([viewable notebook version](https://nbviewer.org/github/greglandrum/rdkit_blog/blob/master/notebooks/Working%20with%20unbalanced%20data%20part%201.ipynb) via nbviewer)\n",
        "\n",
        "    -   approach 2 led to Generalised threshold shifting (GHOST) procedure (could be used for any classification methods) - ghostml package\n",
        "\n",
        "I only used approach 2 here as both approaches were shown to be performing similarly in the paper, and also approach 1 was already described in a RDKit blog post.\n",
        "\n",
        "-   extract the prediction probabilities from the RF classifier trained model\n"
      ],
      "id": "1954d105"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get the positive prediction probabilities of the training set\n",
        "train_probs = rfc.predict_proba(X_train)[:, 1]"
      ],
      "id": "79937306",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6.  Shift decision threshold using ghostml in a postprocessing way (note: last post used re-sampling method in a preprocessing way)\n",
        "\n",
        "-   optimise the decision threshold using ghostml via testing various thresholds (in spaces of 0.05 with range of 0.05 to 0.5) - to search for the most optimal threshold with most maximised Cohen's kappa\n"
      ],
      "id": "8ea12c28"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Setting up different decision thresholds in space of 0.05\n",
        "thresholds = np.round(np.arange(0.05,0.55,0.05),2)\n",
        "thresholds"
      ],
      "id": "d0a8fc14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Looking for the best threshold with the most optimal Cohen's Kappa\n",
        "new_threshold = ghostml.optimize_threshold_from_predictions(y_train, train_probs, thresholds, ThOpt_metrics = 'ROC') "
      ],
      "id": "e4978056",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show the optimal decision threshold\n",
        "new_threshold"
      ],
      "id": "ed22d770",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   calculate confusion matrix and classification metrics based on the optimised decision threshold\n"
      ],
      "id": "093ee46a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Using calc_metrics function again on the newly-found/shifted decision threshold\n",
        "# It showed an improved classification outcome through the confusion matrix\n",
        "calc_metrics(y_train, train_probs, threshold=new_threshold)"
      ],
      "id": "9c574786",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "##### **Plotting ROC curves**\n",
        "\n",
        "Time for some plots - I've shown two different ways to plot ROC curve below.\n",
        "\n",
        "1.  Using *scikit-learn*\n",
        "\n",
        "###### **Testing set ROC curve**\n"
      ],
      "id": "c376ba73"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Testing set ROC curve\n",
        "RocCurveDisplay.from_predictions(y_test, test_probs, plot_chance_level = True)"
      ],
      "id": "dac10fe7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        "###### **Training set ROC curve**\n",
        "\n",
        "This probably looked too good to be true or a textbook-standard ROC curve with AUC at 1.0.\n"
      ],
      "id": "5be7826c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Training set ROC curve\n",
        "RocCurveDisplay.from_predictions(y_train, train_probs, plot_chance_level = True)"
      ],
      "id": "f49a5043",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.  Alternative method using matplotlib which reproduced a similar training set ROC plot:\n"
      ],
      "id": "b0435a7d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Look for true positive rate (tpr), false positive rate (fpr) & threshold\n",
        "fpr, tpr, thresh = metrics.roc_curve(y_train, train_probs)\n",
        "# Plotting\n",
        "plt.figure()\n",
        "# lw = linewidth\n",
        "plt.plot(fpr, tpr, lw = 2)\n",
        "# show random guessing line (threshold = 0.5)\n",
        "plt.plot([0, 1], [0, 1], color = \"g\", lw = 2, linestyle=\"--\")\n",
        "plt.ylim([-0.05, 1.05])\n",
        "plt.xlim([-0.05, 1.0])\n",
        "plt.xlabel(\"specificity\")\n",
        "plt.ylabel(\"recall\")\n",
        "plt.show()"
      ],
      "id": "5ee08f83",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}