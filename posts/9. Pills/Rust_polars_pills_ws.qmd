---
title: Pills dataset - Part 1
subtitle: Web scraping & Polars dataframe library
author: Jennifer HY Lin
date: 2023-1-18
draft: true
categories:
  - Data analytics projects
  - Polars
  - Python
  - Jupyter
jupyter: python3
---

#### **Introduction**

As I've mentioned in my last project that I might look into using Rust interactively via Jupyter lab, I thought to stick to this idea first. So I used evcxr_jupyter. The name, "evcxr", was quite hard to remember at first (pronounced as e-vix-ser according to the author, which I've randomly come across in an online tech interview when I was looking into evcxr), but I've sort of worked out a way to memorise it by taking letters out of "**ev**aluation **c**onte**x**t for **r**ust" (which was what it was called by the author of evcxr in its GitHub repository).

For users of Jupyter Notebook or Lab, they might be quite used to the working speed of the cell outputs in Python kernel. However, one thing I've noticed when I was using evcxr_jupyter or Rust kernel in Jupyter lab was that the speed of cell outputs was noticeably slower (especially at the beginning when loading all dependencies required). The speed improved when loading external crates and modules, and generally it was faster afterward. Due to this reason (note: I did not look into any other optimising strategies for this so this was purely based on the original state of evcxr at the time), I think evcxr was not ideal for a very large and complex data science project at this stage. One thing of note when I was combing through issues in evcxr's GitHub repository was someone mentioned one likely reason of the slow speed could be due to the slow compile time of the Rust compiler, so compile time could be slow in Rust but running speed is blazingly fast. However, Rust, being a systems programming language, still has many advantages such as memory and type safety, fast running speed and concurrency.

Because of the dependency loading speed issue in the Jupyter environment, and also knowing there was already a dataframe library built from Rust, I've opted to use Polars-Python again for the data wrangling part, and then trialled using Rust kernel for a small dataframe for data visualisation by using Plotly.rs. This work would be separated into 3 parts:

-   Part 1: Initial dataset loading and web-scraping
-   Part 2: Data wrangling and mining for data visualisation
-   Part 3: Using Rust for data visualisation

The main reason I wanted to try evcxr was that I could see the potential of using Rust interactively to showcase the results in an relatively fast and efficient manner. This meant specific data exploratory results could reach wider public audience, and provide more impacts in different fields.

```{python}
#pip install --upgrade polars
```

```{python}
import polars as pl
```

```{python}
pl.show_versions()
```

#### **Download dataset**

Dataset was initially spotted and sourced from [Data Is Plural](https://www.data-is-plural.com/archive/2022-11-30-edition/) website (specifically the 2022.11.30 edition). The section I was interested in was the first paragraph at the top, about Pills. By going into one of the links provided in the paragraph, this brought me to the [Pillbox dataset](https://datadiscovery.nlm.nih.gov/Drugs-and-Chemicals/Pillbox-retired-January-28-2021-/crzr-uvwg) from the US National Library of Medicine (NLM), which was actually retired from 28th January 2021, but was still available for downloads, purely for educational or research purposes only. It was **not recommended** for pill identifications (as the dataset was not the most up-to-date version). Alternative resources such as [DailyMed](https://dailymed.nlm.nih.gov/dailymed/) would be more appropriate for readers in the US (for readers in other countries, please consult local health professionals and resources for up-to-date information).

#### **Importing dataset**

When importing pillbox.csv file initially, an error message came up with "...Could not parse '10.16' as dtype Int64 at column 7...". One way to get around this was to add "ignore_errors" to bypass this error first in order to load the dataset. This error could be fixed when checking and converting data types for columns in later steps.

```{python}
df = pl.read_csv("pillbox.csv", ignore_errors = True)
df
```

#### **Data wrangling**

The [Pillbox URL link from NLM](https://datadiscovery.nlm.nih.gov/Drugs-and-Chemicals/Pillbox-retired-January-28-2021-/crzr-uvwg) provided a list of column information for users. Here, we could keep what were needed for our data visualisation in part 2 of the project by extracting a subset of data from the full dataset.

```{python}
print(df.glimpse())
```

Since the plan was for a data visualisation in Rust-evcxr eventually, a relatively simple dataset would be extracted for these pills data. Therefore, only certain columns would be selected for this purpose.

```{python}
df_med = df.select([# shapes of medicines
                    "splshape_text", 
                    # colours of medicines
                    "splcolor_text",
                    # strengths of medicines
                    "spl_strength", 
                    # inactive ingredients/excipients in medicines  
                    "spl_inactive_ing",
                    # dosage forms of medicines e.g. capsules or tablets etc.
                    "dosage_form"]
                  )
df_med
```

#### **Web scraping**

This was not planned initially but I thought it would be a lot easier if I could scrape the dosage form table information from the link I've found through the Pillbox link, as the dosage form column only provided codes! These codes were hard to decipher, and definitely meant something so I searched for it and decided to do a bit of web scraping and also planned to import the web-scraped information as a dataframe, which would be helpful to make this column more meaningful.

```{python}
# web scraping the FDA Dosage Forms URL link 
# https://www.fda.gov/industry/structured-product-labeling-resources/dosage-forms
# to be able to tell what codes correspond to which dosage forms

# Uncomment lines below to install libraries needed for web-scraping
#!pip install requests
#!pip install beautifulsoup4
```

```{python}
import requests
from bs4 import BeautifulSoup
import pandas as pd
```

```{python}
url = "https://www.fda.gov/industry/structured-product-labeling-resources/dosage-forms"
data = requests.get(url)
```

```{python}
soup = BeautifulSoup(data.content, "html.parser")
```

```{python}
print(soup.prettify())
```

```{python}
# Create a file by passing the request content into write () method
# and save the dosage form table as a file in binary format
# for sake of future retrieval of information
with open("FDA_dosage_form", "wb") as file:
    file.write(data.content)
```

#### **Transform web-scraped data into dataframe**

##### **Using Pandas dataframe library**

The original pandas.append() method was going to be deprecated in the future versions of Pandas.

dosage_form = pd.DataFrame(columns = \["Dosage_form", "Code"\])

for row in soup.find_all("tr"): col = row.find_all("td") if (col != \[\]): dosage = col\[0\].text code = col\[1\].text dosage_form = dosage_form.append({"Dosage_form":dosage, "Code":code}, ignore_index = True)

dosage_form

This method might still work currently, however, we could use the pandas.concat() method as shown below.

```{python}
# Using pd.concat() method

# Create an empty dictionary
dict = []

# Create a loop to iterate through the html tags from the soup (web-scraped html content)
# find all html tags that began with <tr>
for row in soup.find_all("tr"):
    # each column would hold the items under the html tags that began with <td>
    col = row.find_all("td")
    if (col != []): 
        # dosage form in column 1
        dosage = col[0].text
        # code in column 2
        code = col[1].text
        # Append each dosage form & code into the dict
        dict.append({"DosageForm": dosage, "dosage_form": code})

# Check if the loop was iterating through the html tags in the soup
# and also it was appending each dosage form & code into the dictionary (uncomment line below)
#print(dict)

# Create an empty dataframe with the column names wanted
dosage_form = pd.DataFrame(columns = ["DosageForm", "dosage_form"])

# Concatenate dosage_form dataframe & the dataframe converted from dict
df_new = pd.concat([dosage_form, pd.DataFrame.from_dict(dict)])

# Print the combined dataframe df_new
df_new
```

```{python}
# Using pd.from_dict() method (a quicker way)

# Create an empty dictionary
dict = []

# Create a loop to iterate through the html tags from the soup (web-scraped html content)
# find all html tags that began with <tr>
for row in soup.find_all("tr"):
    # each column would hold the items under the html tags that began with <td>
    col = row.find_all("td")
    if (col != []): 
        # dosage form in column 1
        dosage = col[0].text
        # code in column 2
        code = col[1].text
        # Append each dosage form & code into the dict
        dict.append({"DosageForm": dosage, "dosage_form": code})

# Check if the loop was working to iterate through the html tags in the soup
# and also it was appending each dosage form & code into the dictionary (uncomment line below)
#print(dict)

# Convert the dictionary into a dataframe
df_new = pd.DataFrame.from_dict(dict)

# Print the dataframe df_new
df_new
```

##### **Using Polars dataframe library**

```{python}
# Using Polars df library

# Create an empty dictionary
dict = []

# Create a loop to iterate through the html tags from the soup (web-scraped html content)
# find all html tags that began with <tr>
for row in soup.find_all("tr"):
    # each column would hold the items under the html tags that began with <td>
    col = row.find_all("td")
    if (col != []): 
        # dosage form in column 1
        dosage = col[0].text
        # code in column 2
        code = col[1].text
        # Append each dosage form & code into the dict
        dict.append({"DosageForm": dosage, "dosage_form": code})

# Check if the loop was iterating through the html tags in the soup
# and that it was also appending each dosage form & code into the dictionary (uncomment line below)
#print(dict)

# Convert dictionary to dataframe
new_df = pl.from_dicts(dict)
new_df
```

#### **Preparation of dataframe for data visualisation**

```{python}
# Join the two dataframes together
df_final = df_med.join(new_df, on = "dosage_form")
# Drop the column dosage_form which had codes of each dosage form 
df_final = df_final.drop("dosage_form")
df_final
```

```{python}
# Save the cleaned dataframe as .csv file
# for use in a new .ipynb file using Rust kernel
df_final.write_csv("pills.csv", sep = ",")
```
