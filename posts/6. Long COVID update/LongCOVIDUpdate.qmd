---
title: "Long COVID - an update"
subtitle: "PDF table scraping, bar graph, interactive map & wordcloud"
author: "Jennifer HY Lin"
date: 2022-09-19
categories: [Data analytics projects, R, Python, Long COVID]
draft: false
---

#### **Background**

This was another update on the current long COVID saga around the world that I thought to follow up from my earlier work (details in the [SQL](https://jhylin.github.io/Data-analytics-and-research-blog/posts/2.%20Long%20COVID%20data%20in%20SQL/LongCOVIDSQL.html) and [Tableau](https://jhylin.github.io/Data-analytics-and-research-blog/posts/3.%20Long%20COVID%20dashboard/TableauDashboard.html) projects). This time the dataset was obtained from another journal paper, which had data collected until July 2021 (the previous paper was only until March 2021). I've used Python to extract a table from the PDF of the paper and also Excel to assist with data cleaning. This was followed by using R to analyse and visualise all the data.

#### **Source of dataset**

Journal paper by Healey Q, Sheikh A, Daines L, Vasileiou E. Symptoms and signs of long COVID: A rapid review and meta-analysis. J Glob Health 2022;12:05014. *Creative Commons Attribution 4.0 International Public License*

#### **Data scraping from PDF**

The dataset was scraped from a PDF obtained via PubMed (journal paper source as shown above) by using tabula-py (*for details please see this post, "[Table scraping from PDF](https://jhylin.github.io/Data_in_life_blog/posts/6.%20Long%20COVID%20update/ExtractTableFromPDF.html)"*). Unfortunately I had trouble installing a similar R package remotely after being archived (tabulizer with known issues in its GitHub repository) so I trialled tabula-py instead. It worked for scraping all the data from the target table, but the downside was that the scraped data did not inherit the original tabular format on PDF, with columns and rows all jumbled. I've discussed a little bit more on the likely reason for this in the blog post link above. So in short, the final scraped table was cleaned in Excel and saved as .csv file, which was then imported as shown below.

#### **Data inspection and wrangling**

```{r echo=TRUE}
# Uncomment below if requiring installations of packages
# install.packages("wordcloud")
# install.packages("RColorBrewer")
# install.packages("tidytext")
# install.packages("leaflet")
```

Loading all the required libraries below. Install libraries needed as shown in codes above.

```{r message=FALSE}
library(tidyverse)
library(ggplot2)
library(knitr)
library(leaflet)
library(tidytext)
library(wordcloud)
library(RColorBrewer)
```

```{r}
df <- read_csv("Full_table.csv")
```

```{r echo=FALSE}
#View(df)
```

Here's a quick overview on the hospitalisation rates across all the studies from this paper.

```{r}
df_hosp <- df %>% 
  select(`Author (country)`, `Hospital (%) {ICU (%)}`)
df_hosp
```

##### **Separating columns and change column type**

The table column of Hospital (%) {ICU (%)} was separated into two separate columns to allow clearer differentiation between hospital and ICU rates within each study. The data type for Hospital (%) column was also changed from character to numeric so we can plot a bar graph later on (otherwise the x-axis may not be accurate or properly shown).

```{r warning=FALSE}
df_hosp_icu <- df_hosp %>% 
  separate(`Hospital (%) {ICU (%)}`, c("Hospital (%)", "ICU (%)"))%>% 
  mutate(across(`Hospital (%)`, as.numeric))
# show the first 10 rows as example
c <- head(df_hosp_icu, 10)
kable(c)
```

##### **Separating rows**

The following shows separating the listed co-morbidities for each study into separate rows, since separating into columns would most likely make the table looking even more complex by adding way too much columns, which would add more difficulties in reading the data.

```{r}
df_new <- df %>% 
  separate_rows(Comorbidities, sep = ", ")
# Show the first 10 rows as example
e <- head(df_new, 10)
kable(e)
```

##### **A frequency count showing types of comorbidities in long COVID**

I have noticed how the comorbidities for each study were listed with different percentages and thought to gather just an initial idea about what types of comorbidities were present in order to see the overall picture. I started by removing these digits and percentage symbols. Obviously since I'm still quite new to R (started using R in July), I soon ran into a problem, I kept on getting stuck with not having the count() function to actually count unique elements under the co-morbidities column.

If you look within the magnified circle on the right in the image below, you'll notice a subtle difference in spacing, so yes the culprit was the **space**[^1] and once it was removed, count() worked nicely as how it should be. One small downside was that it would also remove the space between the co-morbidity terms e.g. "liver disease" became "liverdisease", but since it achieved the aim intended to do a good count on all the co-morbidities, I left it as it was.

[^1]: I swear it's taken me a good half an hour to figure this out... eventually I thought to look at the column itself in order to see if I've missed anything... then voila!

![Screenshot of the extra space(s) in dataframe](Zoom-in%20shot.jpg)

```{r}
df_new %>% 
  # Remove % symbol, numbers and don't forget to remove spaces as well in the column! 
  mutate(Comorbidities = str_remove_all(Comorbidities, "[:digit:]|[%]|[ ]")) %>%
  # Add this line to filter out all the "NA"s
  filter(!is.na(Comorbidities)) %>% 
  # Count the comorbidities in descending order
  count(Comorbidities, sort = TRUE) 
```

Now we can observe the top 3 frequency of all co-morbidities listed were: diabetes, hypertension and IHD[^2]. These were followed by, unsurprisingly, common respiratory illnesses such as asthma, COPD[^3], then obesity, and also CKD[^4], malignancy, dyslipidaemia and so on. These would be considered as high risk factors of developing long COVID symptoms if someone had these co-morbidities present before being infected by the coronoviruses.

[^2]: ischaemic heart disease

[^3]: chronic obstructive pulmonary disease

[^4]: chronic kidney disease

#### **Data visualisations**

##### **Bar graph for hospitalisation rate**

Then a line of code to filter out the results of "NA" under the column of Hospital (%) was added. Most of the cells with "NA" were there to fill the multiple empty row entries for other variables and not for the Hospital (%) column, therefore these "NA"s were removed in this instance. The horizontal bar graph below shows the COVID-19 hospitalisation rate for studies in different countries, presenting a very diverse results between 0% and 100% hospitalisations across all 19 cohort studies.

```{r}
#| fig-cap: 
#|   - "COVID-19 hospitalisation rate across different countries"
df_hosp_icu %>% 
  # filter out all NAs
  filter(!is.na(`Hospital (%)`)) %>% 
  # plot the bar graph
  ggplot(aes(x = `Author (country)`, y = `Hospital (%)`)) + 
  geom_bar(stat = "identity") +
  coord_flip()
```

Note: two of the studies were removed from above, these studies were by Chiesa-Estomba (Italy) and Mahmud (Bangladesh), which had "Not stated" recorded under Hospital (%) {ICU (%)} column. When the Hospital (%) column was converted from character to numeric, these two rows were converted to "NA" automatically.

##### **Interactive map for long COVID results**

###### **Preparing dataframe for map**

```{r warning=FALSE}
df_new_a <- df %>% 
  # separate Author (country) column into two columns 
  # note: rename country as region - needed for joining data later on
  separate(`Author (country)`, c("Author", "region")) %>% 
  # print only the columns as selected
  select(`region`, Results)

# The study author name, Fernandez-de-Las-Penas (Spain), got separated as above
# so replace "de" under Country column with the actual country name of Spain
df_new_a[df_new_a == "de"] <- "Spain" 
# Show first 10 rows as example
d <- head(df_new_a, 10)
kable(d)
```

```{r}
df1 <- df_new_a %>% 
  # re-group dataframe based on region column
  group_by(`region`) %>%
  # merge all rows under Results column into one string
  summarise(across(everything(), ~toString(.)))
df1
```

```{r}
# grab the world map data from ggplot
mapdata <- map_data("world") 
# view full dataset in separate tab 
view(mapdata)
```

```{r}
# combine mapdata dataframe (contains longitudes & latitudes of each country) 
# with df_new_a dataframe (contains country info)
mapdata <- left_join(mapdata, df1, by = "region")
head(mapdata)
```

```{r}
# filter out all the empty or "NA" cells
mapdata_new <- mapdata %>% filter(!is.na(mapdata$Results))
head(mapdata_new)
```

I realised that map_data("world") shows all the longitudes and latitudes for subregions of each country, which after trialling the map visualisation several times (and scratching my head numerous times), I've eventually opted to use centroids of each country stated in the original dataset intead. Otherwise one of the maps I had tested before had countless blobs of circles marking the boundaries of each country!

```{r}
mapdata_final <- mapdata_new %>% 
  group_by(region) %>% 
  # Using centroids of countries = means of longitudes and latitudes for each country
  summarise(long = mean(long), lat = mean(lat))
kable(mapdata_final)
```

```{r}
# join above mapdata_final with the df1 which contains countries and long COVID results
df1_mapdata <- left_join(mapdata_final, df1, by = "region")
kable(df1_mapdata)
```

```{r}
# Prepare pop up information
df1_mapdata <- df1_mapdata %>% 
  # paste region and Results columns into popup_info and add it as a new column into dataset
  # bold texts and add break lines by using html tags as shown
  mutate(popup_info = paste("<b>",region,"</b>","<br/>","<b>","Long COVID symptoms:","</b>","<br/>", Results))
df1_mapdata
```

```{r}
#| fig-cap: 
#|   - "Interactive map for long COVID symptoms"
leaflet() %>% 
  # initialising the graphics environment for map
  addTiles() %>% 
  # add circle markers to map
  # use the df1_mapdata dataset containing countries, longitudes, latitudes and long COVID results
  # add pop up information
  addCircleMarkers(data = df1_mapdata, lat = ~lat, lng = ~long, radius = ~3, popup = ~popup_info)
```

*ABG = arterial blood gas, CT = computed tomography, CRP = c-reactive protein, DLCO = diffusing capacity for carbon monoxide, FBC = full blood count, FEV1 = forced expiratory volume in one second, FVC = forced vital capacity, IL-6 = interleukin-6, LFT = liver function test, LVEF = left ventricular ejection fraction, NT-proBNP = N-terminal pro B-type natriuretic peptide, PTSD = posttraumatic stress disorder, 6MWT = 6-min walk test, U&E = urea and electrolyte, TFT = thyroid function test*

##### **Text mining for word cloud**

When skimming through the Results column, it appeared some of the terms recorded were repetitive, so a wordcloud might be another interesting way to see if it could highlight any particular long COVID symptoms that might jump out from this meta-analysis.

```{r}
# Select the results column
text <- df$Results
# Remove numbers from the texts so that the digits won't appear in the wordcloud
text1 <- str_replace_all(text, "[:digit:]", "")
text1
```

```{r}
text_df <- tibble(line = 1:75, text = text1)
text_df
```

Import tidytext library and tokenise the texts in the selected column.

```{r}
text_df1 <- text_df %>% 
  unnest_tokens(word, text)
text_df1
```

Remove stop_words, count the frequency of appearance of each word, then create a wordcloud.

```{r message=FALSE}
text_df1 %>% 
  anti_join(stop_words) %>% 
  count(word) %>% 
  with(wordcloud(word, n, colors = brewer.pal(8,"Dark2")))
#display.brewer.all to display all colour palettes
```

A known drawback of wordcloud was that the length of a word might influence how big it might appear in the wordcloud, so it was not wholly dependent on the frequency of words in a set of texts. Nevertheless, it was one of the ways to get a rough idea about the most common terms cropping up in collected texts. So this last part was more like a small exercise for anyone who also wanted to try this.

#### **Summary**

Long COVID had shown a very versatile and diverse range of signs and symptoms, often resembling the after-effects from other known viral illnesses, the interactive map above would enable readers to see specific long COVID symptoms for selected countries. People with diabetes, hypertension and IHD might have higher risk of suffering from long COVID if they were infected with the coronoviruses. The types of co-morbidities were not limited to these three unfortunately and several other chronic illnesses mentioned above might also contribute to similar risk. The most affected body systems in long COVID were in respiratory tract, ear, nose and throat areas, musculoskeletal parts, gastrointestinal tract and last, but not the lease, neuropsychiatric systems which could bring fatigue and memory/concentration issue, or more widely known as the "brain fog". All of these outcomes also did not vary widely from earlier meta-analyses on long COVID, reiterating the wide health ramifications that COVID-19 could inflict upon global populations.

##### **Acknowledgement**

I have to thank several online resources when I was trying to build the interactive map. Most notably, I've adapted my codes based on these two useful online resources:

1.  [R tutorial: Creating Maps and mapping data with ggplot2](https://www.youtube.com/watch?v=AgWgPSZ7Gp0) by Dr Paul Christiansen

2.  [Creating interactive maps in R](https://www.youtube.com/watch?v=dx3khWsUO1Y) by A&G Statworks

I also have to thank all the package creators for all the packages used here and all the authors of the journal paper (as mentioned under "Source of dataset") which provided the long COVID data.

```{r}
sessionInfo()
```
