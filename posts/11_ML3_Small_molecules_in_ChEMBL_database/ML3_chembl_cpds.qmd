---
title: Machine learning with small molecules in ChEMBL database 
subtitle: Re-training & final evaluation with *scikit-learn* 
author: Jennifer HY Lin
date: '2023-3-2'
draft: true
categories:
  - Machine learning projects
  - Scikit-learn
  - Python
  - ChEMBL database
  - Cheminformatics
jupyter: python3
---

##### ***Machine learning in drug discovery - series 3***

*Re-training ML model* 
*Final evaluation of ML model*

<br>

##### **Review of ML series 2 & introduction of series 3**


<br>

##### **Import dataframe from ML series 1**

Since *scikit-learn* mainly supports Pandas dataframes for ML, I've opted to use Pandas instead of Polars dataframe library this time, to avoid the extra step of converting a Polars dataframe into a Pandas one.

```{python}
import pandas as pd
```

I've exported the final dataframe from ML series 1 as a .csv file, so that we could continue on this ML series and work on the LR model further. For this ML series 2, the .csv file was imported as shown below.

```{python}
df_ml = pd.read_csv("df_ml.csv")
df_ml.head()
```

```{python}
# Check rows and columns of the df_ml dataframe if needed
#df_ml.shape
```

<br>

##### **Import libraries for machine learning**

```{python}
# Install scikit-learn - an open-source ML library
# Uncomment the line below if needing to install this library
#!pip install -U scikit-learn
```

```{python}
# Import scikit-learn
import sklearn

# Check version of scikit-learn 
print(sklearn.__version__)
```

Other libraries needed to generate ML model were imported as below.

```{python}
# To use NumPy arrays to prepare X & y variables
import numpy as np

# To normalise dataset prior to running ML
from sklearn import preprocessing
# To split dataset into training & testing sets
from sklearn.model_selection import train_test_split

# For data visualisations
# Uncomment line below if requiring to install matplotlib
#!pip install matplotlib
import matplotlib.pyplot as plt
```

<br>

##### **Logistic regression**

To get the LR model ready, we needed to define our X and y variables from the df_ml dataset.

<br>

###### **Defining X and y variables**

```{python}
# Define X variables from df_ml dataset (by selecting certain features)
X = np.asarray(df_ml[["#RO5 Violations", 
                      "QED Weighted", 
                      "CX LogP", 
                      "CX LogD", 
                      "Heavy Atoms"]]
              )
X[0:5]
```

```{python}
# Define y variable
y = np.asarray(df_ml["Max_Phase"])
y[0:5]
```

<br>

###### **Training and testing sets**

```{python}
# Split dataset into training & testing sets

# Random number generator - note: this may produce different result each time
#rng = np.random.RandomState(0) 

# Edited post to use random_state = 250 
# to show comparison with ML series 1 for reproducible result
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 250)
print('Training set:', X_train.shape, y_train.shape)
print('Testing set:', X_test.shape, y_test.shape)
```

<br>

###### **Preprocessing data**

```{python}
# Normalise & clean the dataset
# Fit on the training set - not on testing set as this might lead to data leakage
# Transform on the testing set
X = preprocessing.StandardScaler().fit(X_train).transform(X_test)
X[0:5]
```

<br>

###### **Fitting LR classifier on training set**

One major difference to this LR classifier this time was that cross-validation was included, via using LogisticRegressionCV model, when fitting the training data.

```{python}
# Import logistic regression CV estimator
from sklearn.linear_model import LogisticRegressionCV

# Change to LogisticRegressionCV() - LR with built-in cross validation
# Create an instance of logistic regression CV classifier and fit the data
LogR = LogisticRegressionCV().fit(X_train, y_train)
LogR
```

<br>

###### **Applying LR classifier on testing set for prediction**

```{python}
y_mp = LogR.predict(X_test)
y_mp
```

<br>

###### **Converting predicted values into a dataframe**

```{python}
# Predicted values were based on log odds
# Use describe() method to get characteristics of the distribution
pred = pd.DataFrame(LogR.predict_log_proba(X))
pred.describe()
```

Alternatively, a quicker way to get predicted probabilities was via predict_proba() method in *scikit-learn*.

```{python}
y_mp_proba = LogR.predict_proba(X_test)
# Uncomment below to see the predicted probabilities printed
#print(y_mp_proba)
```

<br>

###### **Converting predicted probabilities into a dataframe**

```{python}
# Use describe() to show distributions
y_mp_prob = pd.DataFrame(y_mp_proba)
y_mp_prob.describe()
```

<br>


<br>

#### **Evaluation of the logistic regression CV model**

###### **Accuracy scores**

```{python}
from sklearn.metrics import accuracy_score
accuracy_score(y_mp, y_test)
```

<br>

###### **Confusion matrix**

Again, to visualise the LR classifer built this time using LogisticRegressionCV estimator in a confusion matrix, the same code was used as previously.

```{python}
# Import confusion matrix from scikit-learn
from sklearn.metrics import confusion_matrix
# Import itertools - functions to create iterators for efficient looping
import itertools

# Function to print and plot confusion matrix
def plot_confusion_matrix(# Sets a cm object (cm = confusion matrix)
                          cm, 
                          # Sets classes of '1s' (Successes) & '0s' (Non-successes) for the cm
                          classes,
                          # If setting normalize = true, reports in ratios instead of counts
                          normalize,
                          title = 'Confusion matrix',
                          # Choose colour of the cm (using colourmap recognised by matplotlib)
                          cmap = plt.cm.Reds):
    
    if normalize:
        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    # Plot the confusion matrix 
    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation = 45)
    plt.yticks(tick_marks, classes)

    # Floats to be round up to two decimal places if using normalize = True
    # or else use integers
    fmt = '.2f' if normalize else 'd'
    # Sets threshold of 0.5
    thresh = cm.max() / 2.
    # Iterate through the results and differentiate between two text colours 
    # by using the threshold as a cut-off
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment = "center",
                 color = "white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
```

```{python}
# Compute confusion matrix
matrix = confusion_matrix(y_test, y_mp, labels = [0,1])
np.set_printoptions(precision = 2)

# Plot confusion matrix without normalisation
plt.figure()
plot_confusion_matrix(matrix, 
                      # Define classes of outcomes
                      classes = ['Max_Phase = 0','Max_Phase = 1'], 
                      # Set normalize = True if wanting ratios instead
                      normalize = False, 
                      title = "Confusion matrix without normalisation"
                     )
```

The four different categories in the confusion matrix were:

-   True positive - Predicted Max_Phase = 1 & True Max_Phase = 1 ( out of samples)
-   True negative - Predicted Max_Phase = 0 & True Max_Phase = 0 ( out of samples)
-   False positive - Predicted Max_Phase = 1 & True Max_Phase = 0 ( out of samples)
-   False negative - Predicted Max_Phase = 0 & True Max_Phase = 1 ( out of samples)

<br>

###### **Classification report**

```{python}
from sklearn.metrics import classification_report
print(classification_report(y_test, y_mp))
```

*Precision* was a measure of the accuracy of a predicted outcome, where a class label had been predicted by the classifier. So in this case, we could see that for class label 1, the precision was 0.72, which corresponded to the true positive result of 128 out of 179 samples (= 0.715). It was defined by:

*Recall*, also known as sensitivity (especially widely used in biostatistics and medical diagnostic fields), was a measure of the strength of the classifier to predict a positive outcome. In simple words, it measured the true positive rate. In this example, there was a total of 128 out of 190 samples (which = 0.674, for True Max_Phase = 1 row) that had a true positive outcome of having a max phase of 1. It was defined by:

The precision and recall metrics could also be calculated for class label = 0, which were shown for the row 0 in the classification report.

*f1-score*, or also known as balanced F-score or F-measure, denoted the harmonic average of both precision and recall metrics. This metric would also give another indication about whether this model performed well on outcome predictions. It normally ranged from 0 (worst precision and recall) to 1 (perfect precision and recall). For this particular classifier, f1-score was at 0.7, which was definitely not at its worst, but also could be further improved. It was defined as:

*Support*, which some readers might have already worked out how the numbers were derived, was the total number of true samples in each class label (read row-wise from the confusion matrix). The main purpose of showing this metric was to help clarifying whether the model or classifier had a reasonably balanced dataset for each class or otherwise.

<br>

###### **Log loss**

Log loss could be used as another gauge to show how good the classifier was at making the outcome predictions. The further the predicted probability was from the true value, the larger the log loss, which was also ranged from 0 to 1. Ideally, the smaller the log loss the better the model would be. Here, we had a log loss of 0.61 for this particular model.

```{python}
# Log loss
from sklearn.metrics import log_loss
log_loss(y_test, y_mp_proba)
```

<br>

#### **Discussions and conclusion**

<br>

#### **Final words**

::: callout-note
Feel free to skip this part as this is really me speaking my thoughts out loud about my situation lately.
:::

After encountering several hiccups during my job hunt in data analytics lately (mainly domestic jobs in NZ), I think I need to make up my mind on which field I'm going for (I might be over-qualified for being a general health data analyst... this was the latest impression/feedback I've received). So here it is, I'm going to concentrate on using machine learning (ML) in Python, specifically in drug discovery field for a while. I'll continue improving the data analytics aspect, but I'll be placing more emphasis on ML so that I can somehow create some ML experiences for myself, while trying to find a good match in the research data science job market in the pharmaceutical field. I also have a feeling that I may need to venture out into overseas roles in the near future, as these types of roles involving ML and pharmaceuticals are just too difficult to spot domestically.

I once read a [blog post on learning ML](https://vickiboykis.com/2022/11/10/how-i-learn-machine-learning/) by V. Boykis, who has suggested to go broadly in topics, then go deep in one of them, which I've agreed wholeheartedly as the approach to go about in the tech world, since there is no way on earth that I will be able to learn everything completely (even OpenAI's ChatGPT has limits - being restricted by the amount and types of input data being fed into the GPT). So since I've branched into 3 programming languages so far, I've decided not to expand further into new programming languages for now, to avoid being "half-bucket-full" for everything, I should really narrow down my focus now. To name the 3 programming languages in the order I've learnt them, they are Python, R and Rust. In that, I'm most comfortable with Python as that is my first language, then it's R, followed by Rust, which is almost brand new. I think right now is a good time for me to go deep into an area that has always caught my attentions.

<br>

#### **References**

-   [scikit-learn documentation](https://scikit-learn.org/stable/index.html)
-   [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa *et al.*, JMLR 12, pp. 2825-2830, 2011.
-   Bruce, P., Bruce, A. & Gedeck P. (2020). Practical statistics for data scientists. O'Reilly.
-   [Stack Overflow](https://stackoverflow.com)
