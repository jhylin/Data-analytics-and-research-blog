{
  "hash": "4d8b99157150b1cb123022d843ec2c8e",
  "result": {
    "markdown": "---\ntitle: Machine learning with small molecules in ChEMBL database\nsubtitle: Re-training & final evaluation with *scikit-learn*\nauthor: Jennifer HY Lin\ndate: 2023-3-2\ndraft: true\ncategories:\n  - Machine learning projects\n  - Scikit-learn\n  - Python\n  - ChEMBL database\n  - Cheminformatics\n---\n\n##### ***Machine learning in drug discovery - series 3***\n\n*Re-training ML model* \n*Final evaluation of ML model*\n\n<br>\n\n##### **Review of ML series 2 & introduction of series 3**\n\n\n<br>\n\n##### **Import dataframe from ML series 1**\n\nSince *scikit-learn* mainly supports Pandas dataframes for ML, I've opted to use Pandas instead of Polars dataframe library this time, to avoid the extra step of converting a Polars dataframe into a Pandas one.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n```\n:::\n\n\nI've exported the final dataframe from ML series 1 as a .csv file, so that we could continue on this ML series and work on the LR model further. For this ML series 2, the .csv file was imported as shown below.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf_ml = pd.read_csv(\"df_ml.csv\")\ndf_ml.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Max_Phase</th>\n      <th>#RO5 Violations</th>\n      <th>QED Weighted</th>\n      <th>CX LogP</th>\n      <th>CX LogD</th>\n      <th>Heavy Atoms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.91</td>\n      <td>2.05</td>\n      <td>0.62</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0.16</td>\n      <td>1.51</td>\n      <td>-0.41</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0.20</td>\n      <td>5.05</td>\n      <td>3.27</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.53</td>\n      <td>3.21</td>\n      <td>3.21</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.14</td>\n      <td>2.80</td>\n      <td>2.80</td>\n      <td>37</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Check rows and columns of the df_ml dataframe if needed\n#df_ml.shape\n```\n:::\n\n\n<br>\n\n##### **Import libraries for machine learning**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Install scikit-learn - an open-source ML library\n# Uncomment the line below if needing to install this library\n#!pip install -U scikit-learn\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Import scikit-learn\nimport sklearn\n\n# Check version of scikit-learn \nprint(sklearn.__version__)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.2.0\n```\n:::\n:::\n\n\nOther libraries needed to generate ML model were imported as below.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# To use NumPy arrays to prepare X & y variables\nimport numpy as np\n\n# To normalise dataset prior to running ML\nfrom sklearn import preprocessing\n# To split dataset into training & testing sets\nfrom sklearn.model_selection import train_test_split\n\n# For data visualisations\n# Uncomment line below if requiring to install matplotlib\n#!pip install matplotlib\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n<br>\n\n##### **Logistic regression**\n\nTo get the LR model ready, we needed to define our X and y variables from the df_ml dataset.\n\n<br>\n\n###### **Defining X and y variables**\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Define X variables from df_ml dataset (by selecting certain features)\nX = np.asarray(df_ml[[\"#RO5 Violations\", \n                      \"QED Weighted\", \n                      \"CX LogP\", \n                      \"CX LogD\", \n                      \"Heavy Atoms\"]]\n              )\nX[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([[ 0.  ,  0.91,  2.05,  0.62, 21.  ],\n       [ 2.  ,  0.16,  1.51, -0.41, 48.  ],\n       [ 2.  ,  0.2 ,  5.05,  3.27, 46.  ],\n       [ 0.  ,  0.53,  3.21,  3.21, 24.  ],\n       [ 1.  ,  0.14,  2.8 ,  2.8 , 37.  ]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Define y variable\ny = np.asarray(df_ml[\"Max_Phase\"])\ny[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([0, 0, 0, 0, 0])\n```\n:::\n:::\n\n\n<br>\n\n###### **Training and testing sets**\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Split dataset into training & testing sets\n\n# Random number generator - note: this may produce different result each time\n#rng = np.random.RandomState(0) \n\n# Edited post to use random_state = 250 \n# to show comparison with ML series 1 for reproducible result\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 250)\nprint('Training set:', X_train.shape, y_train.shape)\nprint('Testing set:', X_test.shape, y_test.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set: (1515, 5) (1515,)\nTesting set: (379, 5) (379,)\n```\n:::\n:::\n\n\n<br>\n\n###### **Preprocessing data**\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Normalise & clean the dataset\n# Fit on the training set - not on testing set as this might lead to data leakage\n# Transform on the testing set\nX = preprocessing.StandardScaler().fit(X_train).transform(X_test)\nX[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([[-0.61846489, -0.79518088,  0.57523481,  0.76170581,  0.47638078],\n       [-0.61846489, -1.24006401,  1.27389185,  1.25867492,  0.37925834],\n       [-0.61846489,  0.4949802 , -0.18352175,  0.12634023, -1.27182321],\n       [-0.61846489, -0.79518088,  0.03433905,  0.28360894, -0.68908855],\n       [-0.61846489, -0.48376269,  0.61655324,  0.79630492, -0.20347633]])\n```\n:::\n:::\n\n\n<br>\n\n###### **Fitting LR classifier on training set**\n\nOne major difference to this LR classifier this time was that cross-validation was included, via using LogisticRegressionCV model, when fitting the training data.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Import logistic regression CV estimator\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# Change to LogisticRegressionCV() - LR with built-in cross validation\n# Create an instance of logistic regression CV classifier and fit the data\nLogR = LogisticRegressionCV().fit(X_train, y_train)\nLogR\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV()</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n<br>\n\n###### **Applying LR classifier on testing set for prediction**\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ny_mp = LogR.predict(X_test)\ny_mp\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\narray([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n       0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0])\n```\n:::\n:::\n\n\n<br>\n\n###### **Converting predicted values into a dataframe**\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Predicted values were based on log odds\n# Use describe() method to get characteristics of the distribution\npred = pd.DataFrame(LogR.predict_log_proba(X))\npred.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>379.000000</td>\n      <td>379.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-1.062787</td>\n      <td>-0.442845</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.235029</td>\n      <td>0.109879</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-2.435645</td>\n      <td>-0.793868</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-1.165361</td>\n      <td>-0.504319</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-1.049435</td>\n      <td>-0.430992</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-0.926131</td>\n      <td>-0.373691</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-0.601649</td>\n      <td>-0.091612</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAlternatively, a quicker way to get predicted probabilities was via predict_proba() method in *scikit-learn*.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ny_mp_proba = LogR.predict_proba(X_test)\n# Uncomment below to see the predicted probabilities printed\n#print(y_mp_proba)\n```\n:::\n\n\n<br>\n\n###### **Converting predicted probabilities into a dataframe**\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Use describe() to show distributions\ny_mp_prob = pd.DataFrame(y_mp_proba)\ny_mp_prob.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>379.000000</td>\n      <td>379.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.482928</td>\n      <td>0.517072</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.172099</td>\n      <td>0.172099</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.009285</td>\n      <td>0.144482</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.372812</td>\n      <td>0.391971</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.511104</td>\n      <td>0.488896</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.608029</td>\n      <td>0.627188</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.855518</td>\n      <td>0.990715</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<br>\n\n\n<br>\n\n#### **Evaluation of the logistic regression CV model**\n\n###### **Accuracy scores**\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_mp, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n0.7150395778364116\n```\n:::\n:::\n\n\n<br>\n\n###### **Confusion matrix**\n\nAgain, to visualise the LR classifer built this time using LogisticRegressionCV estimator in a confusion matrix, the same code was used as previously.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Import confusion matrix from scikit-learn\nfrom sklearn.metrics import confusion_matrix\n# Import itertools - functions to create iterators for efficient looping\nimport itertools\n\n# Function to print and plot confusion matrix\ndef plot_confusion_matrix(# Sets a cm object (cm = confusion matrix)\n                          cm, \n                          # Sets classes of '1s' (Successes) & '0s' (Non-successes) for the cm\n                          classes,\n                          # If setting normalize = true, reports in ratios instead of counts\n                          normalize,\n                          title = 'Confusion matrix',\n                          # Choose colour of the cm (using colourmap recognised by matplotlib)\n                          cmap = plt.cm.Reds):\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    # Plot the confusion matrix \n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 45)\n    plt.yticks(tick_marks, classes)\n\n    # Floats to be round up to two decimal places if using normalize = True\n    # or else use integers\n    fmt = '.2f' if normalize else 'd'\n    # Sets threshold of 0.5\n    thresh = cm.max() / 2.\n    # Iterate through the results and differentiate between two text colours \n    # by using the threshold as a cut-off\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment = \"center\",\n                 color = \"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n```\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Compute confusion matrix\nmatrix = confusion_matrix(y_test, y_mp, labels = [0,1])\nnp.set_printoptions(precision = 2)\n\n# Plot confusion matrix without normalisation\nplt.figure()\nplot_confusion_matrix(matrix, \n                      # Define classes of outcomes\n                      classes = ['Max_Phase = 0','Max_Phase = 1'], \n                      # Set normalize = True if wanting ratios instead\n                      normalize = False, \n                      title = \"Confusion matrix without normalisation\"\n                     )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion matrix, without normalization\n[[147  55]\n [ 53 124]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ML3_chembl_cpds_files/figure-html/cell-19-output-2.png){width=570 height=488}\n:::\n:::\n\n\nThe four different categories in the confusion matrix were:\n\n-   True positive - Predicted Max_Phase = 1 & True Max_Phase = 1 ( out of samples)\n-   True negative - Predicted Max_Phase = 0 & True Max_Phase = 0 ( out of samples)\n-   False positive - Predicted Max_Phase = 1 & True Max_Phase = 0 ( out of samples)\n-   False negative - Predicted Max_Phase = 0 & True Max_Phase = 1 ( out of samples)\n\n<br>\n\n###### **Classification report**\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_mp))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.73      0.73      0.73       202\n           1       0.69      0.70      0.70       177\n\n    accuracy                           0.72       379\n   macro avg       0.71      0.71      0.71       379\nweighted avg       0.72      0.72      0.72       379\n\n```\n:::\n:::\n\n\n*Precision* was a measure of the accuracy of a predicted outcome, where a class label had been predicted by the classifier. So in this case, we could see that for class label 1, the precision was 0.72, which corresponded to the true positive result of 128 out of 179 samples (= 0.715). It was defined by:\n\n*Recall*, also known as sensitivity (especially widely used in biostatistics and medical diagnostic fields), was a measure of the strength of the classifier to predict a positive outcome. In simple words, it measured the true positive rate. In this example, there was a total of 128 out of 190 samples (which = 0.674, for True Max_Phase = 1 row) that had a true positive outcome of having a max phase of 1. It was defined by:\n\nThe precision and recall metrics could also be calculated for class label = 0, which were shown for the row 0 in the classification report.\n\n*f1-score*, or also known as balanced F-score or F-measure, denoted the harmonic average of both precision and recall metrics. This metric would also give another indication about whether this model performed well on outcome predictions. It normally ranged from 0 (worst precision and recall) to 1 (perfect precision and recall). For this particular classifier, f1-score was at 0.7, which was definitely not at its worst, but also could be further improved. It was defined as:\n\n*Support*, which some readers might have already worked out how the numbers were derived, was the total number of true samples in each class label (read row-wise from the confusion matrix). The main purpose of showing this metric was to help clarifying whether the model or classifier had a reasonably balanced dataset for each class or otherwise.\n\n<br>\n\n###### **Log loss**\n\nLog loss could be used as another gauge to show how good the classifier was at making the outcome predictions. The further the predicted probability was from the true value, the larger the log loss, which was also ranged from 0 to 1. Ideally, the smaller the log loss the better the model would be. Here, we had a log loss of 0.61 for this particular model.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# Log loss\nfrom sklearn.metrics import log_loss\nlog_loss(y_test, y_mp_proba)\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\n0.6115648700062126\n```\n:::\n:::\n\n\n<br>\n\n#### **Discussions and conclusion**\n\n<br>\n\n#### **Final words**\n\n\n\n<br>\n\n#### **References**\n\n-   [scikit-learn documentation](https://scikit-learn.org/stable/index.html)\n-   [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa *et al.*, JMLR 12, pp. 2825-2830, 2011.\n-   Bruce, P., Bruce, A. & Gedeck P. (2020). Practical statistics for data scientists. O'Reilly.\n-   [Stack Overflow](https://stackoverflow.com)\n\n",
    "supporting": [
      "ML3_chembl_cpds_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}