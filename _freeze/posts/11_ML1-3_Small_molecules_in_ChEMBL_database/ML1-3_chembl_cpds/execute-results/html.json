{
  "hash": "24585df38b0f2da3e9b114cac41167fe",
  "result": {
    "markdown": "---\ntitle: Small molecules in ChEMBL database\nsubtitle: Series 1.3 - Re-training & re-evaluation with *scikit-learn*\nauthor: Jennifer HY Lin\ndate: 2023-3-17\ndraft: false\ncategories:\n  - Machine learning projects\n  - Scikit-learn\n  - Python\n  - ChEMBL database\n  - Cheminformatics\n---\n\n##### ***Machine learning in drug discovery - series 1.3***\n\n*The ML series has been re-numbered to reflect all 3 posts relevant to small molecules in ChEMBL database to avoid confusions.*\n\n*This post has been updated on 2/9/2023 with a section on \"Data source\" to improve the completeness and reproducibility of this post.*\n\n<br>\n\n##### **Review of ML series 1.2 & introduction of series 1.3**\n\nWithout making this final post for logistic regression ML series for small molecules in ChEMBL database too long, I'll quickly explain what happened in the ML series 1.1 and 1.2. Basically, I've trialled a logistic regression model on a small set of ChEMBL database-extracted small molecule data. The whole purpose was for me to go through the ML series flowchart that I've compiled in ML series 1.2. Currently, this post was ML series 1.3 focusing on re-training the model by using the newly-found parameters to optimise the original model after running the cross-validation and hyper-parameter tuning in ML series 1.2.\n\n<br>\n\n##### **Data source**\n\nThe dataframe used below was based on a .csv file extracted and saved via a direct .csv file download from the homepage of [ChEMBL website](https://www.ebi.ac.uk/chembl/) (via selecting the option of \"Distinct compounds\" containing 2,331,700 compounds at the time for ChEMBL version 31). Data pre-processing and wrangling were done on the same dataframe containing these 2,331,700 compounds (rows of data) via Polars dataframe library. The details of data extraction and wrangling were shown in [ML series 1.1](https://jhylin.github.io/Data_in_life_blog/posts/08_ML1-1_Small_molecules_in_ChEMBL_database/ML1-1_chembl_cpds.html).\n\n<br>\n\n##### **Import dataframe from ML series 1.1**\n\nUsing Pandas again to import the same set of data used in ML series 1.1 and 1.2.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n```\n:::\n\n\nImporting and reading the same df_ml dataset again.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf_ml = pd.read_csv(\"df_ml.csv\")\ndf_ml.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=78}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Max_Phase</th>\n      <th>#RO5 Violations</th>\n      <th>QED Weighted</th>\n      <th>CX LogP</th>\n      <th>CX LogD</th>\n      <th>Heavy Atoms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.91</td>\n      <td>2.05</td>\n      <td>0.62</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0.16</td>\n      <td>1.51</td>\n      <td>-0.41</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0.20</td>\n      <td>5.05</td>\n      <td>3.27</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.53</td>\n      <td>3.21</td>\n      <td>3.21</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.14</td>\n      <td>2.80</td>\n      <td>2.80</td>\n      <td>37</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Check rows and columns of the df_ml dataframe if needed\n#df_ml.shape\n```\n:::\n\n\n<br>\n\n##### **Import libraries for machine learning**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Install scikit-learn - an open-source ML library\n# Uncomment the line below if needing to install this library\n#!pip install -U scikit-learn\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Import scikit-learn\nimport sklearn\n\n# Check version of scikit-learn \nprint(sklearn.__version__)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.2.0\n```\n:::\n:::\n\n\nOther libraries needed were imported as below.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# To use NumPy arrays to prepare X & y variables\nimport numpy as np\n\n# To normalise dataset prior to running ML\nfrom sklearn import preprocessing\n# To split dataset into training & testing sets\nfrom sklearn.model_selection import train_test_split\n\n# For data visualisations\n# Uncomment line below if requiring to install matplotlib\n#!pip install matplotlib\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n<br>\n\n##### **Logistic regression**\n\nHere I've defined the X and y variables again and kept them the same as the ones used in ML series 1.1 and 1.2.\n\n<br>\n\n###### **Defining X and y variables**\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Define X variables from df_ml dataset (by selecting certain features)\nX = np.asarray(df_ml[[\"#RO5 Violations\", \n                      \"QED Weighted\", \n                      \"CX LogP\", \n                      \"CX LogD\", \n                      \"Heavy Atoms\"]]\n              )\nX[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=83}\n```\narray([[ 0.  ,  0.91,  2.05,  0.62, 21.  ],\n       [ 2.  ,  0.16,  1.51, -0.41, 48.  ],\n       [ 2.  ,  0.2 ,  5.05,  3.27, 46.  ],\n       [ 0.  ,  0.53,  3.21,  3.21, 24.  ],\n       [ 1.  ,  0.14,  2.8 ,  2.8 , 37.  ]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Define y variable\ny = np.asarray(df_ml[\"Max_Phase\"])\ny[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=84}\n```\narray([0, 0, 0, 0, 0])\n```\n:::\n:::\n\n\n<br>\n\n###### **Training and testing sets**\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Split dataset into training & testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 250)\nprint('Training set:', X_train.shape, y_train.shape)\nprint('Testing set:', X_test.shape, y_test.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set: (1515, 5) (1515,)\nTesting set: (379, 5) (379,)\n```\n:::\n:::\n\n\n<br>\n\n###### **Preprocessing data**\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Normalise & clean the dataset\n# Fit on the training set - not on testing set as this might lead to data leakage\n# Transform on the testing set\nX = preprocessing.StandardScaler().fit(X_train).transform(X_test)\nX[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=86}\n```\narray([[-0.62, -0.8 ,  0.58,  0.76,  0.48],\n       [-0.62, -1.24,  1.27,  1.26,  0.38],\n       [-0.62,  0.49, -0.18,  0.13, -1.27],\n       [-0.62, -0.8 ,  0.03,  0.28, -0.69],\n       [-0.62, -0.48,  0.62,  0.8 , -0.2 ]])\n```\n:::\n:::\n\n\n<br>\n\n###### **Fitting LR classifier on training set**\n\nOne thing that would be changed this time was that Cs value was changed to 50 (from 10 previously), while using cv = 5, which was already set by default (so this wasn't shown explicitly in the code below). This was because a Cs of 50 and cv of 5, along with a penalty of l2 and solver in lbfgs were found to produce the highest accuracy score last time when we were tuning the hyper-parameters and doing the cross-validations.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Import logistic regression CV estimator\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# Change to LogisticRegressionCV() - LR with built-in cross validation\n# Create an instance of logistic regression CV classifier and fit the data\n# Add in tuned parameters from ML series 1.2\nLogR = LogisticRegressionCV(Cs = 50).fit(X_train, y_train)\nLogR\n```\n\n::: {.cell-output .cell-output-display execution_count=87}\n```{=html}\n<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV(Cs=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegressionCV</label><div class=\"sk-toggleable__content\"><pre>LogisticRegressionCV(Cs=50)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n<br>\n\n###### **Applying LR classifier on testing set for prediction**\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ny_mp = LogR.predict(X_test)\ny_mp\n```\n\n::: {.cell-output .cell-output-display execution_count=88}\n```\narray([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n       0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n       0, 0, 1, 0, 0])\n```\n:::\n:::\n\n\n<br>\n\n###### **Converting predicted values into a dataframe**\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Predicted values were based on log odds\n# Use describe() method to get characteristics of the distribution\npred = pd.DataFrame(LogR.predict_log_proba(X))\npred.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=89}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>379.000000</td>\n      <td>379.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-0.926464</td>\n      <td>-0.556489</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.302448</td>\n      <td>0.218910</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.937517</td>\n      <td>-1.268254</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-1.127372</td>\n      <td>-0.688112</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-0.912437</td>\n      <td>-0.513403</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-0.698208</td>\n      <td>-0.391389</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-0.330342</td>\n      <td>-0.155556</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nTo get predicted probabilities, use predict_proba() method in *scikit-learn*, then the predicted probabilities were converted into a dataframe.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ny_mp_proba = LogR.predict_proba(X_test)\n# Use describe() to show distributions\ny_mp_prob = pd.DataFrame(y_mp_proba)\ny_mp_prob.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=90}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>379.000000</td>\n      <td>379.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.483351</td>\n      <td>0.516649</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.175766</td>\n      <td>0.175766</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.007432</td>\n      <td>0.141781</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.361289</td>\n      <td>0.393433</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.503003</td>\n      <td>0.496997</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.606567</td>\n      <td>0.638711</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.858219</td>\n      <td>0.992568</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<br>\n\n#### **Evaluation of the model after using optimised parameters**\n\n###### **Accuracy scores**\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_mp, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=91}\n```\n0.7018469656992085\n```\n:::\n:::\n\n\nThere were about 0.26% increase in accuracy score when using the LogR model with the tuned parameters from ML series 1.2, when compared with the LogR model made initially in ML series 1.1, which had an accuracy score of 0.6992084432717678.\n\n<br>\n\n###### **Confusion matrix**\n\nAgain, I've used the same confusion matrix function code again to show the new confusion matrix this time.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Import confusion matrix from scikit-learn\nfrom sklearn.metrics import confusion_matrix\n# Import itertools - functions to create iterators for efficient looping\nimport itertools\n\n# Function to print and plot confusion matrix\ndef plot_confusion_matrix(# Sets a cm object (cm = confusion matrix)\n                          cm, \n                          # Sets classes of '1s' (Successes) & '0s' (Non-successes) for the cm\n                          classes,\n                          # If setting normalize = true, reports in ratios instead of counts\n                          normalize,\n                          title = 'Confusion matrix',\n                          # Choose colour of the cm (using colourmap recognised by matplotlib)\n                          cmap = plt.cm.Reds):\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    # Plot the confusion matrix \n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 45)\n    plt.yticks(tick_marks, classes)\n\n    # Floats to be round up to two decimal places if using normalize = True\n    # or else use integers\n    fmt = '.2f' if normalize else 'd'\n    # Sets threshold of 0.5\n    thresh = cm.max() / 2.\n    # Iterate through the results and differentiate between two text colours \n    # by using the threshold as a cut-off\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment = \"center\",\n                 color = \"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Compute confusion matrix\nmatrix = confusion_matrix(y_test, y_mp, labels = [0,1])\nnp.set_printoptions(precision = 2)\n\n# Plot confusion matrix without normalisation\nplt.figure()\nplot_confusion_matrix(matrix, \n                      # Define classes of outcomes\n                      classes = ['Max_Phase = 0','Max_Phase = 1'], \n                      # Set normalize = True if wanting ratios instead\n                      normalize = False, \n                      title = \"Confusion matrix without normalisation\"\n                     )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion matrix, without normalization\n[[141  61]\n [ 52 125]]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](ML1-3_chembl_cpds_files/figure-html/cell-18-output-2.png){width=570 height=488}\n:::\n:::\n\n\n<br>\n\n###### **Classification report**\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_mp))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n           0       0.73      0.70      0.71       202\n           1       0.67      0.71      0.69       177\n\n    accuracy                           0.70       379\n   macro avg       0.70      0.70      0.70       379\nweighted avg       0.70      0.70      0.70       379\n\n```\n:::\n:::\n\n\n<br>\n\n###### **Log loss**\n\nThere was only a slight increase in log loss score this time. The previous log loss was 0.606602645025058, which meant there was only a very small margin of improvement of the model.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# Log loss\nfrom sklearn.metrics import log_loss\nlog_loss(y_test, y_mp_proba)\n```\n\n::: {.cell-output .cell-output-display execution_count=95}\n```\n0.608337653347155\n```\n:::\n:::\n\n\n<br>\n\n#### **Discussions and conclusion**\n\nWithout any surprises, the re-trained model did not provide a huge difference from the initial one, which was already expected in ML series 1.2. However, the purpose of tuning the parameters to reach a more optimised model was still achieved in a very small way, where there was a less than 1% increase in accuracy score and an even smaller increase in log loss value for the re-trained model. The confusion matrix showed a small improvement, with 114 predictions that were false positive and false negatives in the original model and 113 predictions for the same categories in the current model. The classification report basically had very minute change, with only the recall value going up from 0.69 in the old model to 0.70 in the current model in class label zero. Overall, this was not significant enough, given the goal was to improve the model. However, as a learning exercise I think I've learnt something along the way, and hopefully I'll use it to show a much more significant improvement in other future ML scenarios.\n\nOne of the other things I could try next, if I were to use the same set of dataset, was to change the features used to train the model. Max phase, number of rule-of-5 violations, QED weighted scores, CX LogP, CX LogD and the number of heavy atoms for small molecules used in ML series 1.1 to 1.3 might, in fact, not be the best set of features to be used at all. They were mainly randomly chosen, without much thoughts placed into it, however I tried to use QED weighted score as it incorporated several renowned physicochemical factors for small molcules in general, but on the other hand, I could not prove that QED weighted was not biased at all, so some considerations should also be given to this aspect, perhaps more features could be included or changed to see the effect on the model during model evaluation phase. I could also use other types of ML approaches to build the model, which might produce a better prediction model to predict the max phases of small molecules.\n\n<br>\n\n#### **Final words**\n\nCurrently, I haven't thought too much about what my next project will be, as I'm trying to brush up my Python skills further to make sure I understand most of its concepts and usages by practising on LeetCode. There may be a bit of a break before I embark on my next idea. One thing for sure, I'll try to up my experience in ML either with the same set of data in newer version of ChEMBL database (ChEMBL released a version 32 recently) or a different set of data using a different ML approach.\n\n<br>\n\n#### **References**\n\n-   [scikit-learn documentation](https://scikit-learn.org/stable/index.html)\n-   [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa *et al.*, JMLR 12, pp. 2825-2830, 2011.\n-   Bruce, P., Bruce, A. & Gedeck P. (2020). Practical statistics for data scientists. O'Reilly.\n\n",
    "supporting": [
      "ML1-3_chembl_cpds_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}