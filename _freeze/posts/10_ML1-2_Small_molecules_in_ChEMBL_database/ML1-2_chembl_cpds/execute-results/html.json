{
  "hash": "3b30404a996bf223a7b5ab12283ccd3f",
  "result": {
    "markdown": "---\ntitle: Small molecules in ChEMBL database\nsubtitle: Series 1.2 - Cross-validation & hyper-parameter tuning with *scikit-learn*\nauthor: Jennifer HY Lin\ndate: 2023-3-7\ndraft: true\ncategories:\n  - Machine learning projects\n  - Scikit-learn\n  - Python\n  - ChEMBL database\n  - Cheminformatics\n---\n\n##### ***Machine learning in drug discovery - series 1.2***\n\n*The ML series has been re-numbered to reflect all 3 posts relevant to small molecules in ChEMBL database to avoid confusions.*\n\n*This post has been updated on 2/9/2023 with a section on \"Data source\" to improve the completeness and reproducibility of this post.*\n\n<br>\n\n##### **Introduction**\n\nThis work was really a continuation of the first machine learning (ML) in drug discovery series, \"Small molecules in ChEMBL database - Polars dataframe library and machine learning in *scikit-learn*\" (referred to as ML series 1.1 from here onwards). In particular, I wanted to work on the logistics regression (LR) model, and look into other strategies that I could use to improve it. Last time, I used LogisticRegression() method on the df_ml dataframe (df_ml_pd was the actual dataframe name used in ML series 1.1, to denote a conversion from a Polars to Pandas dataframe). I've not changed any parameters for the LR estimator, which meant everything was kept at default settings. Overall, this was an example of a default prototype of a LR classifer, which most likely would be too simplistic and not really reflecting real-world equivalents, but it sort of helped me to think in terms of a LR and ML context.\n\nThis time, with a goal of trying to improve the model, I've planned to use cross-validation and hyper-parameter tuning at least to evaluate the estimator performance. It was also worth evaluating whether LR was the best ML approach for the df_ml dataset, which would likely need to be kept as a separate post to avoid another lengthy read. I also, on the other hand, have had an idea in mind of doing a ML series 1.3 looking at re-training the model and a final evaluation, which would keep me busy in the coming weeks.\n\n::: callout-note\nIn *scikit-learn*, an estimator is alluding to the variety of ML approaches (as one of my interpretations), which are usually grouped into classification (e.g. naive Bayes, logistic regression), regression (e.g. support vector regression), clustering (e.g. K-means clustering) and dimensionality reduction (e.g. principal component analysis). A useful guide to help with choosing the right estimator can be found [here](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html). For a full definition of what an estimator is, refer to this [link](https://scikit-learn.org/stable/glossary.html#term-estimator).\n:::\n\n<br>\n\n###### **Machine learning series 1 - overall plan**\n\nThe overall plan for the ML series for small molecules in ChEMBL database could be visualised through the following flow chart, which was adapted and modified from the section on [cross-validation in *scikit-learn*](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation). This current post was targeting the ML series 1.2 subgraph area.\n\n\n```{mermaid}\n%%{ init: { 'flowchart': { 'curve': 'monotoneY' } } }%%\nflowchart TB\n  subgraph ML series 1.1\n  A[Cleaned dataset] --> B(Train data)\n  A --> C(Test data)\n  end\n  subgraph ML series 1.3\n  B --> D(Re-train model)\n  C --> E[Final evaluation]\n  D --> E\n  end\n  subgraph ML series 1.2\n  B --> G([Cross validation & hyper-parameter tuning])\n  G --> H([Best parameters])\n  F([Parameters]) --> G\n  H --> D\n  end\n```\n\n\n<br>\n\n##### **Data source**\n\nThe dataframe used below was based on a .csv file extracted and saved via a direct .csv file download from the homepage of [ChEMBL website](https://www.ebi.ac.uk/chembl/) (via selecting the option of \"Distinct compounds\" containing 2,331,700 compounds at the time for ChEMBL version 31). Data pre-processing and wrangling were done on the same dataframe containing these 2,331,700 compounds (rows of data) via Polars dataframe library. The details of data extraction and wrangling were shown in [ML series 1.1](https://jhylin.github.io/Data_in_life_blog/posts/08_ML1-1_Small_molecules_in_ChEMBL_database/ML1-1_chembl_cpds.html).\n\n<br>\n\n##### **Import dataframe from ML series 1.1**\n\nSince *scikit-learn* mainly supports Pandas dataframes for ML, I've opted to use Pandas instead of Polars dataframe library this time, to avoid the extra step of converting a Polars dataframe into a Pandas one.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n```\n:::\n\n\nI've exported the final dataframe from ML series 1.1 as a .csv file, so that we could continue on this ML series and work on the LR model further. For this ML series 1.2, the .csv file was imported as shown below.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf_ml = pd.read_csv(\"df_ml.csv\")\ndf_ml.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Max_Phase</th>\n      <th>#RO5 Violations</th>\n      <th>QED Weighted</th>\n      <th>CX LogP</th>\n      <th>CX LogD</th>\n      <th>Heavy Atoms</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.91</td>\n      <td>2.05</td>\n      <td>0.62</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0.16</td>\n      <td>1.51</td>\n      <td>-0.41</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0.20</td>\n      <td>5.05</td>\n      <td>3.27</td>\n      <td>46</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.53</td>\n      <td>3.21</td>\n      <td>3.21</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0.14</td>\n      <td>2.80</td>\n      <td>2.80</td>\n      <td>37</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Check rows and columns of the df_ml dataframe if needed\n#df_ml.shape\n```\n:::\n\n\n<br>\n\n##### **Import libraries for machine learning**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Install scikit-learn - an open-source ML library\n# Uncomment the line below if needing to install this library\n#!pip install -U scikit-learn\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Import scikit-learn\nimport sklearn\n\n# Check version of scikit-learn \nprint(sklearn.__version__)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.5.0\n```\n:::\n:::\n\n\nOther libraries needed to generate ML model were imported as below.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# To use NumPy arrays to prepare X & y variables\nimport numpy as np\n\n# To normalise dataset prior to running ML\nfrom sklearn import preprocessing\n# To split dataset into training & testing sets\nfrom sklearn.model_selection import train_test_split\n```\n:::\n\n\n<br>\n\n##### **Logistic regression**\n\nTo get the LR model ready, the X and y variables were defined with the same sets of physicochemical features from the small molecules in the df_ml dataset.\n\n<br>\n\n###### **Defining X and y variables**\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Define X variables from df_ml dataset\nX = np.asarray(df_ml[[\"#RO5 Violations\", \n                      \"QED Weighted\", \n                      \"CX LogP\", \n                      \"CX LogD\", \n                      \"Heavy Atoms\"]]\n              )\nX[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([[ 0.  ,  0.91,  2.05,  0.62, 21.  ],\n       [ 2.  ,  0.16,  1.51, -0.41, 48.  ],\n       [ 2.  ,  0.2 ,  5.05,  3.27, 46.  ],\n       [ 0.  ,  0.53,  3.21,  3.21, 24.  ],\n       [ 1.  ,  0.14,  2.8 ,  2.8 , 37.  ]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# Define y variable\ny = np.asarray(df_ml[\"Max_Phase\"])\ny[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\narray([0, 0, 0, 0, 0])\n```\n:::\n:::\n\n\n<br>\n\n###### **Training and testing sets**\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n# Split dataset into training & testing sets\n\n# Random number generator - note: this may produce different result each time\n#rng = np.random.RandomState(0) \n\n# Edited post to use random_state = 250 \n# to be the same as ML series 1.1 for reproducible result\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 250)\nprint('Training set:', X_train.shape, y_train.shape)\nprint('Testing set:', X_test.shape, y_test.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set: (1515, 5) (1515,)\nTesting set: (379, 5) (379,)\n```\n:::\n:::\n\n\n<br>\n\n###### **Preprocessing data**\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Normalise & clean the dataset\n# Fit on the training set - not on testing set as this might lead to data leakage\n# Transform on the testing set\nX = preprocessing.StandardScaler().fit(X_train).transform(X_test)\nX[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([[-0.61846489, -0.79518088,  0.57523481,  0.76170581,  0.47638078],\n       [-0.61846489, -1.24006401,  1.27389185,  1.25867492,  0.37925834],\n       [-0.61846489,  0.4949802 , -0.18352175,  0.12634023, -1.27182321],\n       [-0.61846489, -0.79518088,  0.03433905,  0.28360894, -0.68908855],\n       [-0.61846489, -0.48376269,  0.61655324,  0.79630492, -0.20347633]])\n```\n:::\n:::\n\n\n<br>\n\n###### **Fitting LR classifier on training set**\n\nOne major difference to the LR classifier this time was that cross-validation was included by using the LogisticRegressionCV model while fitting the training data.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Import logistic regression CV estimator\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# Change to LogisticRegressionCV() - LR with built-in cross validation\n# Create an instance of logistic regression CV classifier and fit the data\nLogR = LogisticRegressionCV().fit(X_train, y_train)\nLogR\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: black;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: block;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 1ex;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegressionCV()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegressionCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegressionCV.html\">?<span>Documentation for LogisticRegressionCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegressionCV()</pre></div> </div></div></div></div>\n```\n:::\n:::\n\n\n<br>\n\n###### **Applying LogisticRegressionCV classifier on testing set for prediction**\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ny_mp = LogR.predict(X_test)\ny_mp\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\narray([0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1,\n       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n       1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n       1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n       0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n       0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0,\n       0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0,\n       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0,\n       0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n       0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n       0, 0, 1, 0, 0])\n```\n:::\n:::\n\n\n<br>\n\n###### **Converting predicted values into a dataframe**\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Predicted values were based on log odds\n# Use describe() method to get characteristics of the distribution\npred = pd.DataFrame(LogR.predict_log_proba(X))\npred.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>379.000000</td>\n      <td>379.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>-1.062720</td>\n      <td>-0.442881</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.235005</td>\n      <td>0.109890</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-2.435354</td>\n      <td>-0.793951</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-1.165273</td>\n      <td>-0.504365</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>-1.049338</td>\n      <td>-0.431044</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>-0.926061</td>\n      <td>-0.373731</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>-0.601580</td>\n      <td>-0.091640</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAlternatively, a quicker way to get predicted probabilities was via predict_proba() method in *scikit-learn*.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\ny_mp_proba = LogR.predict_proba(X_test)\n# Uncomment below to see the predicted probabilities printed\n#print(y_mp_proba)\n```\n:::\n\n\n<br>\n\n###### **Converting predicted probabilities into a dataframe**\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Use describe() to show distributions\ny_mp_prob = pd.DataFrame(y_mp_proba)\ny_mp_prob.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>379.000000</td>\n      <td>379.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.482928</td>\n      <td>0.517072</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.172099</td>\n      <td>0.172099</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.009286</td>\n      <td>0.144483</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.372810</td>\n      <td>0.391971</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.511115</td>\n      <td>0.488885</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.608029</td>\n      <td>0.627190</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.855517</td>\n      <td>0.990714</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n<br>\n\n##### **Cross-validation & hyper-parameter tuning**\n\n<br>\n\n###### **Cross-validation**\n\nCross-validation was designed to minimise sample loss if all of our datasets were partitioned into three lots for training, testing and validation purposes. Readers might notice an additional set of data for validation here. One of the biggest reasons to add this validation set was that often overfitting could happen on the testing set with testing data being leaked into the model, due to parameter tweaking until the model performed optimally as desired. By having a validation set of the data, this overfitting problem could be avoided.\n\nIn general, model training could take place initially on the training set, with the validation set used for first evaluation, which would be followed by a final evaluation on the testing set if the model testing worked as expected. The \"cross\" part of the cross-validation was the part that described the process of splitting the training data into many smaller number (\"*k*\") of sets, which was also why cross-validation was also known as \"*k*-fold cross-validation\". With the use of *k*-fold cross-validation, the training set was essentially equivalent to *k*-1 of the folds of the training data. The trained model would then be validated by using the remaining parts of the training data, which was almost like being used as a testing data to measure the performance of the trained model.\n\n::: callout-note\nIn short, cross-validation was commonly used as an out-of-sample evaluation metric, where each observation was used for both training and testing, leading to more effective use of data. It was often used for hyper-parameter tuning to avoid overfitting, where parameters could be optimised via grid search techniques (such as GridSearchCV).\n:::\n\n<br>\n\n###### **Decoding LogisticRegressionCV classifier**\n\nSince we've used LogisticRegressionCV classifier for the LR models, this meant it would be unnecessary to use GridSearchCV again according to the definition of the estimatorCV as shown below.\n\nAs quoted from *scikit-learn* on [cross-validation estimator](https://scikit-learn.org/stable/glossary.html#term-cross-validation-estimator):\n\n<q>An estimator that has built-in cross-validation capabilities to automatically select the best hyper-parameters (see the User Guide). Some example of cross-validation estimators are ElasticNetCV and LogisticRegressionCV. Cross-validation estimators are named EstimatorCV and tend to be roughly equivalent to GridSearchCV(Estimator(), ...). The advantage of using a cross-validation estimator over the canonical estimator class along with grid search is that they can take advantage of warm-starting by reusing precomputed results in the previous steps of the cross-validation process. This generally leads to speed improvements. An exception is the RidgeCV class, which can instead perform efficient Leave-One-Out (LOO) CV. By default, all these estimators, apart from RidgeCV with an LOO-CV, will be refitted on the full training dataset after finding the best combination of hyper-parameters.\n\nTherefore, the cross validation part for the LR model was taken care of by the LogisticRegressionCV classifier. However, I wanted to find out more about this particular estimator, so to further dissect LogisticRegressionCV classifer, Stratified K-Folds cross-validator was actually found to be used in its default setting. One of the parameters in the classifier that was closely related to the Stratified K-Folds was the cv parameter. It was a cross-validation generator that could be tuned by providing integers as its equivalent number of folds used. Its default value for LogisticRegressionCV was set as \"None\", which was equivalent to and changed from 3-fold to 5-fold in *scikit-learn* version 0.22.\n\n<br>\n\n###### **Hyper-parameter tuning - how parameters affect LR model**\n\nTo explicitly see the details of all the parameters used after the cross-validation, the names and values of these parameters could be checked for the estimator by using the code below.\n\n``` {{python}}\n# To find the parameters of any ML estimator as suggested by *scikit-learn*\nestimator.get_params()\n```\n\nIn this example, the LR model built was named LogR. All the parameters used for LogR by the LogisticRegressionCV classifer were:\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nLogR.get_params()\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n{'Cs': 10,\n 'class_weight': None,\n 'cv': None,\n 'dual': False,\n 'fit_intercept': True,\n 'intercept_scaling': 1.0,\n 'l1_ratios': None,\n 'max_iter': 100,\n 'multi_class': 'deprecated',\n 'n_jobs': None,\n 'penalty': 'l2',\n 'random_state': None,\n 'refit': True,\n 'scoring': None,\n 'solver': 'lbfgs',\n 'tol': 0.0001,\n 'verbose': 0}\n```\n:::\n:::\n\n\nHowever, by showing this set of parameters used by LogisticRegressionCV classifier wouldn't really tell much about whether any of these parameters were indeed the best ones to fit the model with. So to find out how these parameters influenced the LR model, it was probably best to run a test by using several different parameters on the model to observe the effects. I've had two parameters in mind that I thought would affect the confusion matrix at least - cv and random_state parameters after doing some manual trial and errors by changing the cv and random_state values in the code. However, upon reading and digging further in the online information and resource pools, I quickly realised that cv parameter would probably matter more than random_state parameter. This was based on this line from *scikit-learn* about the LogisticRegressionCV classifer,\n\n<q>For the grid of Cs values and l1_ratios values, the best hyperparameter is selected by the cross-validator StratifiedKFold, but it can be changed using the cv parameter.\n\nSo it appeared that changing cv parameter could affect Cs and l1_ratios values as well. Also from *scikit-learn* documentation on LR, other parameters that could be tuned were:\n\n-   ***solvers*** - algorithms used in classifiers\n\n-   ***penalties (or regularisation)*** - aims to reduce model generalisation errors and regulate or prevent overfitting\n\n-   ***C*** - controls regularisation or penalty strengths (which would be already taken care of in this case if using LogisticRegressionCV classifier).\n\n![Summary table for different penalties supported by different solvers in logistic regression, adapted from *scikit-learn* - https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression. Note: only certain penalties and solvers work together, OVR = One-vs-Rest.](LR_penalties_solvers.jpg)\n\nA few other online projects or tutorials using logistic regression in *scikit-learn* had also mentioned that logistic regression in general did not have a lot of key hyper-parameters for tuning. Another post I happened to bump into even concluded that better time should be used to link the model results with the actual business metrics instead, rather than trying to use hyper-parameter tuning on the LR model. Nevertheless, I still wanted to see how these parameters would affect the LR model in this case, even if it was of minor significance, so that I would fully understand how all of them would work together, and how tuning hyper-parameters would be like.\n\nIn order to search and test the LR parameters on the different models that would be generated in the test, I've opted to use RepeatedStratifiedKFold as the cross-validation method (which was the default cross-validator method used in LogisticRegressionCV classifier). The \"Repeated\" version of it would repeat Stratified K-Fold at the stated (*n*) times. Because of this, GridSearchCV would then be used to exhaustively search for all the best parameters in this case, with the aim to see how the changes in parameters would affect the accuracy scores for each model.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n# Re-sampled y variable randomly so that there were same numbers of samples as X variables\ny = np.asarray(df_ml[\"Max_Phase\"].sample(n = 379, random_state = 250))\ny.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n(379,)\n```\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# Code adapted from: https://machinelearningmastery.com/hyperparameters-for-classification-machine-learning-algorithms/\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Set up LR model to test (the estimatorCV version)\nmodel = LogisticRegressionCV()\n\n# Set up parameters to test\n# Note: default value for cv = 5-fold\ncv = [5, 10, 20, 30]\n# Note: default value for Cs = 10 (integers or floats only)\nCs = [1, 10, 50, 100]\n# Note: default solver = \"lbfgs\"\nsolvers = [\"lbfgs\", \"liblinear\", \"newton-cg\", \"newton-cholesky\"]\n# \"sag\", \"saga\" not used as the dataset used here was small\n# they were mainly used for large datasets for speed\npenalty = [\"l2\"]\n\n# Specify grid for parameters to test in grid search\ngrid = dict(cv = cv, Cs = Cs, solver = solvers, penalty = penalty)\n\n# Specify type of cross-validation method to be used\nCV = RepeatedStratifiedKFold(n_splits = 5, n_repeats = 2, random_state = 2)\n\n# Set up grid search\n# Specify grid search parameters\ngrid_search = GridSearchCV(\n  # Specify model\n  estimator = model,\n  # Specify parameters to test\n  param_grid = grid,\n  # Number of jobs to run in parallel \n  # 1 = no parallel jobs; \n  # None = unset, but could be interpreted as \"1\" unless otherwise specified; \n  # -1 = all processors used\n  n_jobs = -1,\n  # Type of cross-validation method to be used \n  # (if none set, default 5-fold cv will be used)\n  cv = CV, \n  # Type of scoring to be used to evaluate the model\n  scoring = \"accuracy\",\n  # Value to assign to the \"scoring\" of the model \n  # if an error occurs during model fitting\n  error_score = 0\n  )\n  \n# fit the grid search on X and y variables\ngrid_result = grid_search.fit(X, y)\n\n# Results with means & standard deviations of accuracy scores \n# with parameters used\nprint(\"Best mean test score: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstd = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\nfor mean, stdv, param in zip(means, std, params):\n    print(\"%f (%f) with: %r\" % (mean, stdv, param))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest mean test score: 0.699263 using {'Cs': 50, 'cv': 5, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 5, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.670281 (0.055017) with: {'Cs': 1, 'cv': 5, 'penalty': 'l2', 'solver': 'liblinear'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 5, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 5, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.670281 (0.055017) with: {'Cs': 1, 'cv': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 10, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 20, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.670281 (0.055017) with: {'Cs': 1, 'cv': 20, 'penalty': 'l2', 'solver': 'liblinear'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 20, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 20, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 30, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.670281 (0.055017) with: {'Cs': 1, 'cv': 30, 'penalty': 'l2', 'solver': 'liblinear'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 30, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.532982 (0.005887) with: {'Cs': 1, 'cv': 30, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.697930 (0.041930) with: {'Cs': 10, 'cv': 5, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.696614 (0.041302) with: {'Cs': 10, 'cv': 5, 'penalty': 'l2', 'solver': 'liblinear'}\n0.697930 (0.041930) with: {'Cs': 10, 'cv': 5, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.697930 (0.041930) with: {'Cs': 10, 'cv': 5, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.696614 (0.041720) with: {'Cs': 10, 'cv': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.696614 (0.041720) with: {'Cs': 10, 'cv': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n0.696614 (0.041720) with: {'Cs': 10, 'cv': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.696614 (0.041720) with: {'Cs': 10, 'cv': 10, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.693982 (0.051953) with: {'Cs': 10, 'cv': 20, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.699246 (0.039988) with: {'Cs': 10, 'cv': 20, 'penalty': 'l2', 'solver': 'liblinear'}\n0.693982 (0.051953) with: {'Cs': 10, 'cv': 20, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.693982 (0.051953) with: {'Cs': 10, 'cv': 20, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.695298 (0.041882) with: {'Cs': 10, 'cv': 30, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.695298 (0.041882) with: {'Cs': 10, 'cv': 30, 'penalty': 'l2', 'solver': 'liblinear'}\n0.695298 (0.041882) with: {'Cs': 10, 'cv': 30, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.695298 (0.041882) with: {'Cs': 10, 'cv': 30, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.699263 (0.043226) with: {'Cs': 50, 'cv': 5, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.699263 (0.043226) with: {'Cs': 50, 'cv': 5, 'penalty': 'l2', 'solver': 'liblinear'}\n0.699263 (0.043226) with: {'Cs': 50, 'cv': 5, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.699263 (0.043226) with: {'Cs': 50, 'cv': 5, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.696614 (0.040455) with: {'Cs': 50, 'cv': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.697930 (0.041095) with: {'Cs': 50, 'cv': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n0.697930 (0.041095) with: {'Cs': 50, 'cv': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.697930 (0.041095) with: {'Cs': 50, 'cv': 10, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.692667 (0.049633) with: {'Cs': 50, 'cv': 20, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.693982 (0.049566) with: {'Cs': 50, 'cv': 20, 'penalty': 'l2', 'solver': 'liblinear'}\n0.692667 (0.049633) with: {'Cs': 50, 'cv': 20, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.692667 (0.049633) with: {'Cs': 50, 'cv': 20, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.692667 (0.042897) with: {'Cs': 50, 'cv': 30, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.692667 (0.042897) with: {'Cs': 50, 'cv': 30, 'penalty': 'l2', 'solver': 'liblinear'}\n0.693982 (0.043222) with: {'Cs': 50, 'cv': 30, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.693982 (0.043222) with: {'Cs': 50, 'cv': 30, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.696632 (0.042453) with: {'Cs': 100, 'cv': 5, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.695298 (0.041047) with: {'Cs': 100, 'cv': 5, 'penalty': 'l2', 'solver': 'liblinear'}\n0.696632 (0.042453) with: {'Cs': 100, 'cv': 5, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.696632 (0.042453) with: {'Cs': 100, 'cv': 5, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.697930 (0.040672) with: {'Cs': 100, 'cv': 10, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.696614 (0.040455) with: {'Cs': 100, 'cv': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n0.697930 (0.040672) with: {'Cs': 100, 'cv': 10, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.697930 (0.040672) with: {'Cs': 100, 'cv': 10, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.695298 (0.051521) with: {'Cs': 100, 'cv': 20, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.693982 (0.049566) with: {'Cs': 100, 'cv': 20, 'penalty': 'l2', 'solver': 'liblinear'}\n0.693982 (0.050944) with: {'Cs': 100, 'cv': 20, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.693982 (0.050944) with: {'Cs': 100, 'cv': 20, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n0.697930 (0.042747) with: {'Cs': 100, 'cv': 30, 'penalty': 'l2', 'solver': 'lbfgs'}\n0.692667 (0.042897) with: {'Cs': 100, 'cv': 30, 'penalty': 'l2', 'solver': 'liblinear'}\n0.695298 (0.040623) with: {'Cs': 100, 'cv': 30, 'penalty': 'l2', 'solver': 'newton-cg'}\n0.695298 (0.040623) with: {'Cs': 100, 'cv': 30, 'penalty': 'l2', 'solver': 'newton-cholesky'}\n```\n:::\n:::\n\n\n<br>\n\n#### **Results & discussions**\n\nTrends observed from the grid search above (when penalty = l2):\n\n-   For Cs = 1, the best accuracy score was 0.670281 across all 4 different cv parameters (5, 10, 20, 30), and the best solver was liblinear\n\n-   For Cs = 10, the best accuracy score was 0.699246 for cv = 20 and when solver was set as liblinear\n\n-   For Cs = 50, the best accuracy score was 0.699263 for cv = 5 across all 4 solvers\n\n-   For Cs = 100, the best accuracy score was 0.697930 for cv = 10 across 3 out of 4 solvers only, which were lbfgs, newton-cg and newton-cholesky\n\nIt appeared that for smaller values of Cs, liblinear might be more suitable than the default lbfgs solver. However, for higher values of Cs, e.g. 50 and above, liblinear might not always be the best solver. The values of Cs and cv parameters that generated the best mean accuracy score were 50 and 5 respectively. The best mean accuracy score produced was 0.699263 with a standard deviation of 0.043226. Solver-wise, there were actually 3 other solvers, liblinear, newton-cg and newton-cholesky, along with the default lbfgs that generated the same mean accuracy score and standard deviations while using Cs = 50 and cv = 5. In my initial LogisticRegressionCV model, I used a different value of Cs parameter (at 10), but with the same cv and penalty parameters.\n\nTherefore, for the next ML series 1.3, the plan was to re-train the LogisticRegression model with the newly-discovered best parameters and re-evaluate the model to see if there would be any particular differences. Although currently I suspected the differences might be small (which probably also echoed other ML work on different datasets that also used LR), since the accuracy scores generated from this grid search and ML series 1.1 were very similar. However, the goal of this post was to understand how cross-validation could be used for hyper-parameter tuning to find the optimal parameters to avoid overfitting a ML model, and this was likely more applicable in other ML approaches.\n\n<br>\n\n#### **Final words**\n\n::: callout-note\nFeel free to skip this final part as this was really me speaking my thoughts out loud about my portfolio lately.\n:::\n\nI once read a [blog post on learning ML](https://vickiboykis.com/2022/11/10/how-i-learn-machine-learning/), which has suggested to go broadly in topics, then go deep in one of them, which I've agreed wholeheartedly as the approach to go about in the tech world, since there are no ways on earth to learn absolutely everything completely (even OpenAI's ChatGPT has limits - being restricted by the amount and types of input data being fed into the GPT). So, since I've branched into 3 programming languages so far, I've decided not to expand further into new programming languages for now, to avoid being \"half-bucket-full\" for everything, I should really narrow down my focus now. To name the 3 programming languages in the order I've learnt them, they are Python, R and Rust. In that, I'm most comfortable with Python as that is my first language, then it's R, followed by Rust, which is almost brand new. I think right now is a good time for me to go deep into an area that has always caught my attentions. So I'll be concentrating more on ML in my portfolio in the near future.\n\n<br>\n\n#### **References**\n\n-   [scikit-learn documentation](https://scikit-learn.org/stable/index.html) - particularly on LogisticRegressionCV classifier and GridSearchCV\n-   [Scikit-learn: Machine Learning in Python](https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html), Pedregosa *et al.*, JMLR 12, pp. 2825-2830, 2011.\n-   Bruce, P., Bruce, A. & Gedeck P. (2020). Practical statistics for data scientists. O'Reilly.\n-   [Stack Overflow](https://stackoverflow.com)\n\n",
    "supporting": [
      "ML1-2_chembl_cpds_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}