{
  "hash": "44160b9b336b38902055cbd8a432fd87",
  "result": {
    "markdown": "---\ntitle: Random forest\nsubtitle: Series 2.2 - building model\nauthor: Jennifer HY Lin\ndate: '2023-10-11'\ndraft: true\ncategories:\n  - Machine learning projects\n  - Tree models\n  - Pandas\n  - Scikit-learn\n  - ChEMBL database\n  - Python\nformat: html\nbibliography: references.bib\n---\n\n##### **What is a random forest?**\n\nThe decision tree model built last time was purely based on one model on its own, which often might not be as accurate as we've hoped for. If we're thinking along the line mathematically or statistically to improve the model, we would then think about using the average of multiple models [@breiman1998] to see if this output would better reflect the real-life scenario. This model averaging approach was in fact constantly used in our lives with an example such as using majority votes in elections or decision-making processes.\n\nThis concept was also used in random forest [@breiman2001], which as the name suggested, was composed of many tree models (decision trees), forming a forest. The \"random\" part was introduced by two ways. The first one was via using bootstrap samples, which was also known as bagging or bootstrap aggregating [@bruce2020], where samples were drawn with replacements within the training datasets for each tree built in the ensemble (the perturb-and-combine technique [@breiman1998]). While bootstrap sampling was happening, randomness was also incorporated at the same time into the training sets. The second way randomness was introduced was by using a random subset of features for splitting at the nodes, or a full set of features could also be used instead - the main goal was to achieve best splits at each node.\n\n\n-   Look into random forest regression particularly\n-   Two main types as classifier & regressor with differences e.g. class labels (binary) or continuous variables\n\n\n*Draft plan*: \n- Many other options available in Scikit-learn ensemble methods e.g. voting classifier/regressor or stacking models to reduce biases \n\n- ?Likely using same dataset from series 2.1 \n\n- ?possibly one post only \n\n- Scikit-learn RandomForestRegressor() \n\n- array X (no. of samples, no. of features) vs. array y (no. of samples or target values) \n\n- Parameter tuning likely needed \n\n- Plots - Black-box ML (unlike white-box ML for decision tree)\n\n",
    "supporting": [
      "1_model_build_files"
    ],
    "filters": [],
    "includes": {}
  }
}