{
  "hash": "5dc8c0e6c66863cc0db5c5c5e766573c",
  "result": {
    "markdown": "---\ntitle: Random forest\nsubtitle: 'Series 2.2 - model building, imbalanced dataset, feature importances & hyperparameter tuning'\nauthor: Jennifer HY Lin\ndate: '2023-11-22'\ndraft: false\ncategories:\n  - Machine learning projects\n  - Tree models\n  - Pandas\n  - Scikit-learn\n  - ChEMBL database\n  - Python\nformat: html\nbibliography: references.bib\n---\n\n##### **Quick overview of this post**\n\n-   Short introduction of random forest\n-   Random forest methods or classes in *scikit-learn*\n-   Random forest regressor model in *scikit-learn*\n-   Training and testing data splits\n    -   ChEMBL-assigned max phase splits\n    -   Imbalanced learning regression and max phase splits\n-   Scoring metrics of trained models\n-   Feature importances in dataset\n    -   feature_importances_attribute in *scikit-learn*\n    -   permutation_importance function in *scikit-learn*\n    -   SHAP approach\n-   Hyperparameter tuning on number of trees\n\n<br>\n\n##### **What is a random forest?**\n\nThe [decision tree model built last time](https://jhylin.github.io/Data_in_life_blog/posts/16_ML2-1_Decision_tree/3_model_build.html) was purely based on one model on its own, which often might not be as accurate or reflective in real-life. To improve the model, the average outcome from multiple models [@breiman1998] should be considered to see if this would provide a more realistic image. This model averaging approach was also constantly used in our daily lives, for example, using majority votes during decision-making steps.\n\nThe same model averaging concept was also used in random forest [@breiman2001], which as the name suggested, was composed of many decision trees (models) forming a forest. Each tree model would be making its own model prediction. By accruing multiple predictions since we have multiple trees, the average obtained from these predictions would produce one single result in the end. The advantage of this was that it improved the accuracy of the prediction by reducing variances, and also minimised the problem of overfitting the model if it was purely based on one model only (more details in section 1.11.2.1. Random Forests from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#random-forests)).\n\nThe \"random\" part of the random forest was introduced in two ways. The first one was via using bootstrap samples, which was also known as bagging or bootstrap aggregating [@bruce2020], where samples were drawn with replacements within the training datasets for each tree built in the ensemble (also known as the perturb-and-combine technique [@breiman1998]). While bootstrap sampling was happening, randomness was also incorporated into the training sets at the same time. The second way randomness was introduced was by using a random subset of features for splitting at the nodes, or a full set of features could also be used (although this was generally not recommended). The main goal here was to achieve best splits at each node.\n\n<br>\n\n##### **Random forest in *scikit-learn***\n\n*Scikit-learn* had two main types of random forest classes - [ensemble.RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [ensemble.RandomForestRegressor()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor). When to use which class would depend on the target values. The easiest thing to do was to decide whether the target variables had class labels (binary types or non-continuous variables e.g. yes or no, or other different categories to be assigned) or continuous (numerical) variables, which in this case, if I were to continue using the same dataset from the decision tree series, it would be a continuous variable or feature, pKi, the inhibition constant.\n\nThere were also two other alternative random forest methods in *scikit-learn*, which were ensemble.RandomTreesEmbedding() and ensemble.ExtraTreesClassifier() or ensemble.ExtraTreesRegressor(). The difference for RandomTreesEmbedding() was that it was an unsupervised method that used data transformations (more details from section 1.11.2.6. on \"Totally Random Trees Embedding\" in [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#totally-random-trees-embedding)). On the other side, there was also an option to use ExtraTreesClassifier() or ExtraTreesRegressor() to generate extremely randomised trees that would go for another level up in randomness (more deatils in section 1.11.2.2. on Extremely Randomized Trees from [*scikit-learn*](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees)). The main difference for this type of random forest was that while there was already a random subset of feature selection used (with an intention to select the most discerning features), more randomness were added on top of this by using purely randomly generated splitting rules for picking features at the nodes. The advantage of this type of method was that it would reduce variance and increase the accuracy of the model, but the downside was there might be an increase in bias within the model.\n\n<br>\n\n##### **Building a random forest regressor model using *scikit-learn***\n\nAs usual, all the required libraries were imported first.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\n\n# For imbalanced datasets in regression \nimport ImbalancedLearningRegression as iblr\n\n# Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n# Feature importances\n# Permutation_importance\nfrom sklearn.inspection import permutation_importance\n# SHAP values\nimport shap\n\n# Hyperparameter tuning\nfrom sklearn.model_selection import cross_val_score, RepeatedKFold\n\nfrom numpy import mean, std\nfrom natsort import index_natsorted\nimport numpy as np\n\n# Showing version of scikit-learn used\nprint(sklearn.__version__)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.3.2\n```\n:::\n:::\n\n\nImporting dataset that was preprocessed from last time - link to data source: [first decision tree post](https://jhylin.github.io/Data_in_life_blog/posts/16_ML2-1_Decision_tree/1_data_col_prep.html).\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = pd.read_csv(\"ache_2d_chembl.csv\")\ndata.drop(columns = [\"Unnamed: 0\"], inplace=True)\n# Preparing data for compounds with max phase with \"NaN\" by re-labelling to \"null\"\ndata[\"max_phase\"].fillna(\"null\", inplace=True)\ndata.head()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nSetting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value 'null' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>molecule_chembl_id</th>\n      <th>pKi</th>\n      <th>max_phase</th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CHEMBL60745</td>\n      <td>8.787812</td>\n      <td>null</td>\n      <td>245.041526</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CHEMBL208599</td>\n      <td>10.585027</td>\n      <td>null</td>\n      <td>298.123676</td>\n      <td>0.388889</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>21</td>\n      <td>...</td>\n      <td>4.331775</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CHEMBL95</td>\n      <td>6.821023</td>\n      <td>4.0</td>\n      <td>198.115698</td>\n      <td>0.307692</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>15</td>\n      <td>...</td>\n      <td>2.014719</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CHEMBL173309</td>\n      <td>7.913640</td>\n      <td>null</td>\n      <td>694.539707</td>\n      <td>0.666667</td>\n      <td>8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>8</td>\n      <td>50</td>\n      <td>...</td>\n      <td>2.803680</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CHEMBL1128</td>\n      <td>6.698970</td>\n      <td>4.0</td>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n<br>\n\n##### **Training/testing splits**\n\nTwo approaches were used, where one was based purely on max phase split (between max phases null and 4), which was used last time in the decision tree series, and the other one was using the same max phase split but with an ImbalancedLearningRegression method added on top of it.\n\n<br>\n\n###### **Preparing training data using max phase split**\n\nX variable was set up first from the dataframe, and then converted into a NumPy array, which consisted of the number of samples and number of features. This was kept the same as how it was in the decision tree posts.\n\n::: callout-note\nIt's usually recommended to copy the original data or dataframe before doing any data manipulations to avoid unnecessary changes to the original dataset (this was not used in the decision tree posts, but since I'm going to use the same set of data again I'm doing it here.)\n:::\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# X variables (molecular features)\n# Make a copy of the original dataframe first\ndata_mp4 = data.copy()\n# Selecting all max phase 4 compounds\ndata_mp4 = data_mp4[data_mp4[\"max_phase\"] == 4]\nprint(data_mp4.shape)\ndata_mp4.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(10, 25)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>molecule_chembl_id</th>\n      <th>pKi</th>\n      <th>max_phase</th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>CHEMBL95</td>\n      <td>6.821023</td>\n      <td>4.0</td>\n      <td>198.115698</td>\n      <td>0.307692</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>15</td>\n      <td>...</td>\n      <td>2.014719</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CHEMBL1128</td>\n      <td>6.698970</td>\n      <td>4.0</td>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>CHEMBL640</td>\n      <td>6.000000</td>\n      <td>4.0</td>\n      <td>235.168462</td>\n      <td>0.461538</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>17</td>\n      <td>...</td>\n      <td>1.791687</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>CHEMBL502</td>\n      <td>7.688246</td>\n      <td>4.0</td>\n      <td>379.214744</td>\n      <td>0.458333</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>28</td>\n      <td>...</td>\n      <td>2.677222</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>CHEMBL481</td>\n      <td>7.296709</td>\n      <td>4.0</td>\n      <td>586.279135</td>\n      <td>0.515152</td>\n      <td>10</td>\n      <td>1</td>\n      <td>7</td>\n      <td>10</td>\n      <td>43</td>\n      <td>...</td>\n      <td>3.632560</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# Select molecular features for X array (n_samples, n_features)\nX_mp4_df = data_mp4[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]\n\nprint(X_mp4_df.shape)\nX_mp4_df.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(10, 22)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>n_rotatable_bonds</th>\n      <th>n_radical_electrons</th>\n      <th>tpsa</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>198.115698</td>\n      <td>0.307692</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>15</td>\n      <td>0</td>\n      <td>0</td>\n      <td>38.91</td>\n      <td>...</td>\n      <td>2.014719</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>2</td>\n      <td>0</td>\n      <td>20.23</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>235.168462</td>\n      <td>0.461538</td>\n      <td>4</td>\n      <td>3</td>\n      <td>1</td>\n      <td>4</td>\n      <td>17</td>\n      <td>6</td>\n      <td>0</td>\n      <td>58.36</td>\n      <td>...</td>\n      <td>1.791687</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>379.214744</td>\n      <td>0.458333</td>\n      <td>4</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>28</td>\n      <td>6</td>\n      <td>0</td>\n      <td>38.77</td>\n      <td>...</td>\n      <td>2.677222</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>131</th>\n      <td>586.279135</td>\n      <td>0.515152</td>\n      <td>10</td>\n      <td>1</td>\n      <td>7</td>\n      <td>10</td>\n      <td>43</td>\n      <td>4</td>\n      <td>0</td>\n      <td>114.20</td>\n      <td>...</td>\n      <td>3.632560</td>\n      <td>0</td>\n      <td>4</td>\n      <td>4</td>\n      <td>1</td>\n      <td>2</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# Convert X_mp4_df to numpy array\nX_mp4 = X_mp4_df.to_numpy()\nX_mp4\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([[ 1.98115698e+02,  3.07692308e-01,  2.00000000e+00,\n         2.00000000e+00,  3.00000000e+00,  2.00000000e+00,\n         1.50000000e+01,  0.00000000e+00,  0.00000000e+00,\n         3.89100000e+01,  7.06488238e-01,  2.69580000e+00,\n         2.01471913e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n         2.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 2.01092042e+02,  4.00000000e-01,  2.00000000e+00,\n         1.00000000e+00,  1.00000000e+00,  3.00000000e+00,\n         1.30000000e+01,  2.00000000e+00,  0.00000000e+00,\n         2.02300000e+01,  6.08112327e-01, -1.01700000e+00,\n         3.18586632e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 2.35168462e+02,  4.61538462e-01,  4.00000000e+00,\n         3.00000000e+00,  1.00000000e+00,  4.00000000e+00,\n         1.70000000e+01,  6.00000000e+00,  0.00000000e+00,\n         5.83600000e+01,  7.31539693e-01,  1.34040000e+00,\n         1.79168720e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 3.79214744e+02,  4.58333333e-01,  4.00000000e+00,\n         0.00000000e+00,  4.00000000e+00,  4.00000000e+00,\n         2.80000000e+01,  6.00000000e+00,  0.00000000e+00,\n         3.87700000e+01,  7.47461492e-01,  4.36110000e+00,\n         2.67722173e+00,  1.00000000e+00,  1.00000000e+00,\n         2.00000000e+00,  2.00000000e+00,  0.00000000e+00,\n         2.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n         1.00000000e+00],\n       [ 5.86279135e+02,  5.15151515e-01,  1.00000000e+01,\n         1.00000000e+00,  7.00000000e+00,  1.00000000e+01,\n         4.30000000e+01,  4.00000000e+00,  0.00000000e+00,\n         1.14200000e+02,  3.55955569e-01,  4.09110000e+00,\n         3.63256044e+00,  0.00000000e+00,  4.00000000e+00,\n         4.00000000e+00,  1.00000000e+00,  2.00000000e+00,\n         3.00000000e+00,  0.00000000e+00,  2.00000000e+00,\n         2.00000000e+00],\n       [ 5.10461822e+02,  8.00000000e-01,  6.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  6.00000000e+00,\n         3.60000000e+01,  2.10000000e+01,  0.00000000e+00,\n         2.76900000e+01,  2.05822189e-01,  5.45250000e+00,\n         3.25765349e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 1.84066459e+02,  1.00000000e+00,  3.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  5.00000000e+00,\n         1.10000000e+01,  4.00000000e+00,  0.00000000e+00,\n         3.55300000e+01,  6.29869319e-01,  2.91400000e+00,\n         3.34514393e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 2.87152144e+02,  5.29411765e-01,  4.00000000e+00,\n         1.00000000e+00,  4.00000000e+00,  4.00000000e+00,\n         2.10000000e+01,  1.00000000e+00,  0.00000000e+00,\n         4.19300000e+01,  8.00524269e-01,  1.85030000e+00,\n         4.22684283e+00,  1.00000000e+00,  2.00000000e+00,\n         3.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 3.48142697e+02,  3.68421053e-01,  2.00000000e+00,\n         0.00000000e+00,  3.00000000e+00,  4.00000000e+00,\n         2.30000000e+01,  5.00000000e+00,  0.00000000e+00,\n         6.48000000e+00,  7.09785317e-01,  5.44140000e+00,\n         4.22359068e+00,  0.00000000e+00,  1.00000000e+00,\n         1.00000000e+00,  2.00000000e+00,  0.00000000e+00,\n         2.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00],\n       [ 2.34092376e+02,  3.07692308e-01,  2.00000000e+00,\n         2.00000000e+00,  3.00000000e+00,  3.00000000e+00,\n         1.60000000e+01,  0.00000000e+00,  0.00000000e+00,\n         3.89100000e+01,  7.60853221e-01,  3.11760000e+00,\n         3.21871482e+00,  1.00000000e+00,  0.00000000e+00,\n         1.00000000e+00,  1.00000000e+00,  1.00000000e+00,\n         2.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n         0.00000000e+00]])\n```\n:::\n:::\n\n\nAgain, y variable was arranged via the dataframe as well, and converted into a NumPy array. It consisted of the number of samples only as this was the target variable.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# y array (n_samples) - target outcome pKi\ny_mp4_df = data_mp4[\"pKi\"]\ny_mp4_df\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n2      6.821023\n4      6.698970\n6      6.000000\n9      7.688246\n131    7.296709\n133    4.431798\n160    5.221849\n171    6.522879\n180    4.607303\n195    6.995679\nName: pKi, dtype: float64\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# Convert y_mp4_df to numpy array\ny_mp4 = y_mp4_df.to_numpy()\ny_mp4\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([6.82102305, 6.69897   , 6.        , 7.68824614, 7.29670862,\n       4.43179828, 5.22184875, 6.52287875, 4.60730305, 6.99567863])\n```\n:::\n:::\n\n\n<br>\n\n###### **Training model using max phase split only**\n\nBoth X and y variables were used to fit the RandomForestRegressor() estimator.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# n_estimators = 100 by default\n# note: if wanting to use whole dataset - switch off \"bootstrap\" parameter by using \"False\"\nrfreg = RandomForestRegressor(max_depth=3, random_state=1, max_features=0.3)\nrfreg.fit(X_mp4, y_mp4)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=3, max_features=0.3, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=3, max_features=0.3, random_state=1)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n<br>\n\n###### **Preparing testing data using max phase split only**\n\nTesting data was mainly based on compounds with max phase assigned as \"0\" or \"null\" after I renamed it above.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndata_mp_null = data.copy()\n# Selecting all max phase \"null\" compounds\ndata_mp_null = data_mp_null[data_mp_null[\"max_phase\"] == \"null\"]\nprint(data_mp_null.shape)\ndata_mp_null.head() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(466, 25)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>molecule_chembl_id</th>\n      <th>pKi</th>\n      <th>max_phase</th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CHEMBL60745</td>\n      <td>8.787812</td>\n      <td>null</td>\n      <td>245.041526</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CHEMBL208599</td>\n      <td>10.585027</td>\n      <td>null</td>\n      <td>298.123676</td>\n      <td>0.388889</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>21</td>\n      <td>...</td>\n      <td>4.331775</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CHEMBL173309</td>\n      <td>7.913640</td>\n      <td>null</td>\n      <td>694.539707</td>\n      <td>0.666667</td>\n      <td>8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>8</td>\n      <td>50</td>\n      <td>...</td>\n      <td>2.803680</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>CHEMBL102226</td>\n      <td>4.698970</td>\n      <td>null</td>\n      <td>297.152928</td>\n      <td>0.923077</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>18</td>\n      <td>...</td>\n      <td>2.965170</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>CHEMBL103873</td>\n      <td>5.698970</td>\n      <td>null</td>\n      <td>269.121628</td>\n      <td>0.909091</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n      <td>16</td>\n      <td>...</td>\n      <td>3.097106</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Set up X test variable with the same molecular features\nX_mp_test_df = data_mp_null[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]\n\n# Convert X test variables from df to arrays\nX_mp_test = X_mp_test_df.to_numpy()\n\nX_mp_test\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([[2.45041526e+02, 4.00000000e-01, 2.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [2.98123676e+02, 3.88888889e-01, 2.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [6.94539707e+02, 6.66666667e-01, 8.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [3.11152144e+02, 3.15789474e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [3.68096076e+02, 9.23076923e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 2.00000000e+00, 2.00000000e+00],\n       [2.46136828e+02, 5.00000000e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 3.00000000e+00, 3.00000000e+00]])\n```\n:::\n:::\n\n\n<br>\n\n###### **Training/testing splits using ImbalancedLearningRegression and max phase splits**\n\nI didn't really pay a lot of attentions when I was doing data splits in the decision tree series, as my main focus was on building a single tree in order to fully understand and see what could be derived from just one tree. Now, when I reached this series on random forest, I realised I forgot to mention in the last series that data splitting was actually very crucial on model performance and could influence outcome predictions. It could also become quite complicated as more approaches were available to split the data. Also, the way the data was splitted could produce different outcomes.\n\nAfter I've splitted the same dataset based on compounds' max phase assignments in ChEMBL and also fitted the training data on the random forest regressor, I went back and noticed that the training and testing data were very imbalanced and I probably should do something about it before fitting them onto another model.\n\nAt this stage, I went further to look into whether imbalanced datasets should be addressed in regression tasks, and did a surface search online. So based on common ML concensus, addressing imbalanced datasets were more applicable to classification tasks (e.g. binary labels or multi-class labels), rather than regression problems. However, recent ML research looked into the issue of imbalanced datasets in regression. This [blog post](https://neptune.ai/blog/how-to-deal-with-imbalanced-classification-and-regression-data) mentioned a few studies that looked into this type of problem, and I thought they were very interesting and worth a mention at least. One of them that I've looked into was SMOTER, which was based on synthetic minority over-sampling technique (SMOTE)[@chawla2002], and was named this way because it was basically a SMOTE for regression (hence SMOTER)[@torgo2013]. Synthetic minority over-sampling technique for regression with Gaussian noise (SMOGN)[@smogn] was another technique that was built upon SMOTER, but with Gaussian noises added. This has subsequently led me to ImbalancedLearningRegression library [@wu2022imbalancedlearningregression], which was a variation of SMOGN. This was the one used on my imbalanced dataset, shown in the section below.\n\nA simple flow diagram was drawn below showing the evolution of different techniques when dealing with imbalanced datasets in classification (SMOTE) and regression (SMOTER, SMOGN and ImbalancedLearningRegression):\n\n\n```{mermaid}\nflowchart LR\n  A(SMOTE) --> B(SMOTER)\n  B --> C(SMOGN)\n  C --> D(ImbalancedLearningRegression)\n```\n\n\nGitHub repository for ImbalancedLearningRegression package is available [here](https://github.com/paobranco/ImbalancedLearningRegression), with its documentation available [here](https://imbalancedlearningregression.readthedocs.io/en/latest/intro.html).\n\nAlso, I just wanted to mention that these were not the only techniques available for treating imbalanced datasets in regression, as there were other ones in the literature and most likely more are being developed currently, but I only had time to cover these here for now.\n\nI also would like to mention another really useful open-source resource for treating imbalanced datasets in classifications since I did not use it in this post due to the problem being more of a regression one than a classification one - [imbalance-learn library](https://imbalanced-learn.org/stable/index.html#).\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Original dataset - checking shape again\nprint(data.shape)\ndata.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(481, 25)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>molecule_chembl_id</th>\n      <th>pKi</th>\n      <th>max_phase</th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CHEMBL60745</td>\n      <td>8.787812</td>\n      <td>null</td>\n      <td>245.041526</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CHEMBL208599</td>\n      <td>10.585027</td>\n      <td>null</td>\n      <td>298.123676</td>\n      <td>0.388889</td>\n      <td>2</td>\n      <td>2</td>\n      <td>4</td>\n      <td>3</td>\n      <td>21</td>\n      <td>...</td>\n      <td>4.331775</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CHEMBL95</td>\n      <td>6.821023</td>\n      <td>4.0</td>\n      <td>198.115698</td>\n      <td>0.307692</td>\n      <td>2</td>\n      <td>2</td>\n      <td>3</td>\n      <td>2</td>\n      <td>15</td>\n      <td>...</td>\n      <td>2.014719</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CHEMBL173309</td>\n      <td>7.913640</td>\n      <td>null</td>\n      <td>694.539707</td>\n      <td>0.666667</td>\n      <td>8</td>\n      <td>0</td>\n      <td>2</td>\n      <td>8</td>\n      <td>50</td>\n      <td>...</td>\n      <td>2.803680</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CHEMBL1128</td>\n      <td>6.698970</td>\n      <td>4.0</td>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n      <td>13</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 25 columns</p>\n</div>\n```\n:::\n:::\n\n\nSo my little test on using ImbalancedLearningRegression package started from below.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\niblr_data = data.copy()\n\n# Introducing Gaussian noise for data sampling\ndata_gn = iblr.gn(data = iblr_data, y = \"pKi\", pert = 1)\nprint(data_gn.shape)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:   0%|          | 0/58 [00:00<?, ?it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  14%|#3        | 8/58 [00:00<00:00, 78.04it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  31%|###1      | 18/58 [00:00<00:00, 88.18it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  48%|####8     | 28/58 [00:00<00:00, 90.68it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  66%|######5   | 38/58 [00:00<00:00, 91.06it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix:  83%|########2 | 48/58 [00:00<00:00, 91.38it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix: 100%|##########| 58/58 [00:00<00:00, 93.01it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rsynth_matrix: 100%|##########| 58/58 [00:00<00:00, 90.95it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rr_index:   0%|          | 0/8 [00:00<?, ?it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\rr_index: 100%|##########| 8/8 [00:00<00:00, 302.21it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n(480, 25)\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# Followed by max phase split, where max phase 4 = training dataset\ndata_gn_mp4 = data_gn[data_gn[\"max_phase\"] == 4]\ndata_gn_mp4\nprint(data_gn_mp4.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(7, 25)\n```\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# Also splitted max phase null compounds = testing dataset\ndata_gn_mp_null = data_gn[data_gn[\"max_phase\"] == \"null\"]\ndata_gn_mp_null\nprint(data_gn_mp_null.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(470, 25)\n```\n:::\n:::\n\n\nThere were several different sampling techniques in ImbalancedLearningRegression package. I've only tried random over-sampling, under-sampling and Gaussian noise, but there were also other ones such as SMOTE and ADASYN (in over-sampling technique) or condensed nearest neighbor, Tomeklinks and edited nearest neightbour (in under-sampling technique) that I haven't used.\n\nRandom over-sampling actually oversampled the max phase null compounds (sample size increased), while keeping all 10 max phase 4 compounds. Under-sampling removed all of the max phase 4 compounds (which was most likely not the best option, since I was aiming to use them as training compounds), with max phase null compounds also reduced in size too. Due to post length, I did not show the code for random over-sampling and under-sampling, but for people who are interested, I think it would be interesting to test them out.\n\nI ended up using Gauissian noise sampling and it reduced max phase 4 compounds slightly, and increased the max phase null compounds a little bit too, which seemed to be the most balanced data sampling at the first try. (Note: as stated from the documentation for ImbalancedLearningRegression package, missing values within features would be removed automatically, I've taken care of this in my last series of posts so no difference were observed here.)\n\nThe change in the distribution of pKi values for the Gaussian noise sampling method between the original and sample-modified datasets could be seen in the kernel density estimate plot below. The modified dataset had a flatter target density curve than the original density plot, which was more concentrated and peaked between pKi values of 6 and 8. The range of pKi values for the ten max phase 4 compounds collected was between 4 and 8.\n\n[*Plot reference*](https://github.com/paobranco/ImbalancedLearningRegression/blob/master/examples/Gaussian_noise.ipynb)\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Quick look at how the pKi values differed \n# after applying Gaussian noise sampling to dataset\n# Plot target variable, pKi distributions\nsns.kdeplot(data[\"pKi\"], label = \"Original\")\nsns.kdeplot(data_gn[\"pKi\"], label = \"Modified\")\nplt.legend(labels = [\"Original\", \"Modified\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n<matplotlib.legend.Legend at 0x1379571c0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-16-output-2.png){width=597 height=429}\n:::\n:::\n\n\nNext, the modified ImbalancedLearningRegression-Gaussian noise (iblr-gn) training data was converted into a NumPy array.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\n# Select molecular features for X variable\nX_mp4_gn_df = data_gn_mp4[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]\n\nprint(X_mp4_gn_df.shape)\nX_mp4_gn_df.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(7, 22)\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=16}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mw</th>\n      <th>fsp3</th>\n      <th>n_lipinski_hba</th>\n      <th>n_lipinski_hbd</th>\n      <th>n_rings</th>\n      <th>n_hetero_atoms</th>\n      <th>n_heavy_atoms</th>\n      <th>n_rotatable_bonds</th>\n      <th>n_radical_electrons</th>\n      <th>tpsa</th>\n      <th>...</th>\n      <th>sas</th>\n      <th>n_aliphatic_carbocycles</th>\n      <th>n_aliphatic_heterocyles</th>\n      <th>n_aliphatic_rings</th>\n      <th>n_aromatic_carbocycles</th>\n      <th>n_aromatic_heterocyles</th>\n      <th>n_aromatic_rings</th>\n      <th>n_saturated_carbocycles</th>\n      <th>n_saturated_heterocyles</th>\n      <th>n_saturated_rings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>241</th>\n      <td>348.142697</td>\n      <td>0.368421</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>23.0</td>\n      <td>5.0</td>\n      <td>0</td>\n      <td>6.48</td>\n      <td>...</td>\n      <td>4.223591</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>264</th>\n      <td>510.461822</td>\n      <td>0.800000</td>\n      <td>6.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>36.0</td>\n      <td>21.0</td>\n      <td>0</td>\n      <td>27.69</td>\n      <td>...</td>\n      <td>3.257653</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>277</th>\n      <td>235.168462</td>\n      <td>0.461538</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>1.0</td>\n      <td>4.0</td>\n      <td>17.0</td>\n      <td>6.0</td>\n      <td>0</td>\n      <td>58.36</td>\n      <td>...</td>\n      <td>1.791687</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>305</th>\n      <td>379.214744</td>\n      <td>0.458333</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>28.0</td>\n      <td>6.0</td>\n      <td>0</td>\n      <td>38.77</td>\n      <td>...</td>\n      <td>2.677222</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>344</th>\n      <td>201.092042</td>\n      <td>0.400000</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>3.0</td>\n      <td>13.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>20.23</td>\n      <td>...</td>\n      <td>3.185866</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 22 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nX_mp4_gn = X_mp4_gn_df.to_numpy()\n```\n:::\n\n\nSimilarly, this was also applied to the target y variable in the iblr-gn dataset.\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\n# y variable (target outcome - pKi)\ny_mp4_gn_df = data_gn_mp4[\"pKi\"]\n\ny_mp4_gn = y_mp4_gn_df.to_numpy()\ny_mp4_gn\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([4.60730305, 4.43179828, 6.        , 7.68824614, 6.69897   ,\n       6.99567863, 6.82102305])\n```\n:::\n:::\n\n\nThen the iblr-gn training data were fitted onto another random forest regressor model.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\n# n_estimators = 100 by default\n# note: if wanting to use whole dataset - switch off \"bootstrap\" parameter by using \"False\"\nrfreg_gn = RandomForestRegressor(max_depth=3, random_state=1, max_features=0.3)\nrfreg_gn.fit(X_mp4_gn, y_mp4_gn)\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=3, max_features=0.3, random_state=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=3, max_features=0.3, random_state=1)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nModified iblr-gn testing data were also prepared and converted into a NumPy array.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n# Set up X test variable with the same molecular features\nX_mp_gn_test_df = data_gn_mp_null[['mw', 'fsp3', 'n_lipinski_hba', 'n_lipinski_hbd', 'n_rings', 'n_hetero_atoms', 'n_heavy_atoms', 'n_rotatable_bonds', 'n_radical_electrons', 'tpsa', 'qed', 'clogp', 'sas', 'n_aliphatic_carbocycles', 'n_aliphatic_heterocyles', 'n_aliphatic_rings', 'n_aromatic_carbocycles', 'n_aromatic_heterocyles', 'n_aromatic_rings', 'n_saturated_carbocycles', 'n_saturated_heterocyles', 'n_saturated_rings']]\n\n# Convert X test variables from df to arrays\nX_mp_gn_test = X_mp_gn_test_df.to_numpy()\n\nX_mp_gn_test\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\narray([[5.80224119e+02, 2.64705882e-01, 7.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [3.77210327e+02, 3.91304348e-01, 5.00000000e+00, ...,\n        0.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n       [5.24297368e+02, 4.54545455e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [1.94999014e+02, 1.25000000e-01, 4.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [4.99247107e+02, 3.66666667e-01, 7.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [2.42051384e+02, 0.00000000e+00, 3.00000000e+00, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])\n```\n:::\n:::\n\n\n<br>\n\n###### **Using trained model for prediction on testing data**\n\nPredicting max phase-splitted data only.\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\n# Predict pKi values for the compounds with \"null\" max phase\n# using the training model rfreg \n# Uncomment code below to print prediction result\n#print(rfreg.predict(X_mp_test))\n\n# or use:\ny_mp_test = rfreg.predict(X_mp_test)\n```\n:::\n\n\nPredicting iblr-gn data with max phase splits.\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ny_mp_gn_test = rfreg_gn.predict(X_mp_gn_test)\n```\n:::\n\n\n<br>\n\n###### **Scoring and metrics of trained models**\n\nChecking model accuracy for both training and testing datasets was recommended to take place before moving onto discovering feature importances. A *scikit-learn* explanation for this could be found in the section on [\"Permutation feature importance\"](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance). So the accuracy scores for the model were shown below.\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\n# Training set accuracy\nprint(f\"Random forest regressor training accuracy: {rfreg.score(X_mp4, y_mp4):.2f}\")\n\n# Testing set accuracy\nprint(f\"Random forest regressor testing accuracy: {rfreg.score(X_mp_test, y_mp_test):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom forest regressor training accuracy: 0.86\nRandom forest regressor testing accuracy: 1.00\n```\n:::\n:::\n\n\nIt looked like both the training and testing accuracies for the random forest regressor model (rfreg) were quite high, meaning that the model was able to remember the molecular features well from the training set (the tiny sample of 10 compounds), and the model was able to apply them to the testing set (which should contain about 400 or so compounds) as well, in order to make predictions on the target value of pKi. This has somewhat confirmed that the model was indeed making predictions, rather than not making any predictions at all, which meant there might be no point in finding out which features were important in the data. Therefore, we could now move onto processing the feature importances to fill in the bigger story i.e. which features were more pivotal towards influencing pKi values of approved drugs targeting acetylcholinesterase (AChE).\n\nSimilar model accuracy scores were also generated for the iblr-gn modified dataset, which appeared to follow a similar pattern as the max phase-splitted dataset.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\n# iblr-Gaussian noise & max phase splitted data\n# Training set accuracy\nprint(f\"Random forest regressor training accuracy: {rfreg_gn.score(X_mp4_gn, y_mp4_gn):.2f}\")\n\n# Testing set accuracy\nprint(f\"Random forest regressor testing accuracy: {rfreg_gn.score(X_mp_gn_test, y_mp_gn_test):.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRandom forest regressor training accuracy: 0.82\nRandom forest regressor testing accuracy: 1.00\n```\n:::\n:::\n\n\nNow, setting up the y_true, which was the acutal pKi values of the testing set, and were converted into a NumPy array too.\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ny_true = data_mp_null[\"pKi\"]\ny_true = y_true.to_numpy(copy=True)\n```\n:::\n\n\nI also found out the mean squared error (MSE) between y_true (actual max phase null compounds' pKi values) and y_pred (predicted max phase null compounds' pKi values). When MSE was closer to zero, the better the model was, meaning less errors were present.\n\nSome references that might help with explaining MSE:\n\n- [*scikit-learn* link](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error)\n- [Stats StackExchange link](https://stats.stackexchange.com/questions/579755/whats-a-reasonable-mean-squared-error-or-rmse)\n- [blog post link](https://statisticsbyjim.com/regression/mean-squared-error-mse/) \n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\n# For max phase splitted dataset only\nmean_squared_error(y_true, y_mp_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\n2.3988097789702505\n```\n:::\n:::\n\n\nWhen R^2^ (coefficient of determination) was closer to 1, the better the model is, with a usual range between 0 and 1 [@bruce2020]. If it was negative, then the model might not be performing as well as expected. However, there could be exceptions as other model evaluation methods should also be interpreted together with R^2^ (a poor R^2^ might not be wholly indicating it's a poor model).\n\nSome references that might help with understanding R^2^:\n\n- [Stats StackExchange link](https://stats.stackexchange.com/questions/414349/is-my-model-any-good-based-on-the-diagnostic-metric-r2-auc-accuracy-rmse)\n- [*scikit-learn* link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html#sklearn.metrics.r2_score)\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n# For max phase splitted dataset only\nr2_score(y_true, y_mp_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\n-0.16228227953132635\n```\n:::\n:::\n\n\nBecause the data was re-sampled in a iblr-gn way, the size of array would be different from the original dataset, so here I've specifically grabbed pKi values from the iblr-gn modified data to get the actual pKi values for the max phase null compounds.\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\ny_true_gn = data_gn_mp_null[\"pKi\"]\ny_true_gn = y_true_gn.to_numpy(copy=True)\n```\n:::\n\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\n# MSE for iblr-gn data\nmean_squared_error(y_true_gn, y_mp_gn_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\n7.113575446065506\n```\n:::\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\n# R squared for iblr-gn data\nr2_score(y_true_gn, y_mp_gn_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n-1.086375640337942\n```\n:::\n:::\n\n\nWell, it appeared iblr-gn dataset might not offer much advantage than the original max phase splitted method. However, even the max phase splitted method wasn't that great either, but it might still be interesting to find out which features were important in relation to the pKi values.\n\n<br>\n\n##### **Feature importances**\n\nThere were two types of feature importances available in *scikit-learn*, which I've described below. I've also added a Shapley additive explanations (SHAP) approach to this section as well to show different visualisation styles for feature importances on the same set of data.\n\n<br>\n\n###### **feature_importances\\_ attribute from *scikit-learn***\n\nThe impurity-based feature importances (also known as Gini importance) were shown below.\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\n# Compute feature importances on rfreg training model\nfeature_imp = rfreg.feature_importances_\n```\n:::\n\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\n# Check what feature_imp looks like (an array)\nfeature_imp\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\narray([0.0396524 , 0.06608031, 0.01129602, 0.09707957, 0.03625411,\n       0.04393835, 0.05287614, 0.03339396, 0.        , 0.14225505,\n       0.03562854, 0.16945464, 0.04450665, 0.02810539, 0.01437198,\n       0.0220527 , 0.02136604, 0.00803076, 0.04866529, 0.        ,\n       0.044468  , 0.04052409])\n```\n:::\n:::\n\n\nI decided to write a function to convert a NumPy array into a plot below as this was also needed in the next section.\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n# Function to convert array to df leading to plots \n# - for use in feature_importances_ & permutation_importance\n\ndef feat_imp_plot(feat_imp_array, X_df):\n\n    \"\"\"\n    Function to convert feature importance array into a dataframe, \n    which is then used to plot a bar graph \n    to show the feature importance ranking in the random forest model for the dataset used.\n\n    feat_imp_array is the array obtained from the feature_importances_ attribute, \n    after having a estimator/model fitted.\n\n    X_df is the dataframe for the X variable, \n    where the feature column names will be used in the plot.\n    \"\"\"\n\n    # Convert the feat_imp array into dataframe\n    feat_imp_df = pd.DataFrame(feat_imp_array)\n    \n    # Obtain feature names via column names of dataframe\n    # Rename the index as \"features\"\n    feature = X_df.columns.rename(\"features\")\n\n    # Convert the index to dataframe\n    feature_name_df = feature.to_frame(index = False)\n\n    # Concatenate feature_imp_df & feature_name_df\n    feature_df = pd.concat(\n        [feat_imp_df, feature_name_df], \n        axis=1\n        ).rename(\n            # Rename the column for feature importances\n            columns = {0: \"feature_importances\"}\n            ).sort_values(\n                # Sort values of feature importances in descending order\n                \"feature_importances\", ascending=False\n                )\n    \n    # Seaborn bar plot\n    sns.barplot(\n        feature_df, \n        x = \"feature_importances\", \n        y = \"features\")\n```\n:::\n\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n# Testing feat_imp_plot function\nfeat_imp_plot(feature_imp, X_mp4_df)\n```\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-35-output-1.png){width=732 height=429}\n:::\n:::\n\n\nAn alternative way to plot was via Matplotlib directly (note: Seaborn was built based on Matplotlib, so the plots were pretty similar). The code below were probably a bit more straightforward but without axes named and the values were not sorted (only as an example but more code could be added to do this).\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\n# Matplotlib plot\nfrom matplotlib import pyplot as plt\nplt.barh(X_mp4_df.columns, rfreg.feature_importances_)\n```\n\n::: {.cell-output .cell-output-display execution_count=35}\n```\n<BarContainer object of 22 artists>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-36-output-2.png){width=714 height=411}\n:::\n:::\n\n\n<br>\n\n###### **permutation_importance function from *scikit-learn***\n\nThere were known issues with the built-in feature_importances\\_ attribute in *scikit-learn*. As quoted from *scikit-learn* on [feature importance evaluation](https://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation):\n\n> ... The impurity-based feature importances computed on tree-based models suffer from two flaws that can lead to misleading conclusions. First they are computed on statistics derived from the training dataset and therefore do not necessarily inform us on which features are most important to make good predictions on held-out dataset. Secondly, they favor high cardinality features, that is features with many unique values. Permutation feature importance is an alternative to impurity-based feature importance that does not suffer from these flaws. ...\n\nSo I've also tried the permutation_importance function (a model-agnostic method).\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nperm_result = permutation_importance(rfreg, X_mp_test, y_mp_test, n_repeats=10, random_state=1, n_jobs=2)\n\n# Checking data type of perm_result\ntype(perm_result)\n```\n\n::: {.cell-output .cell-output-display execution_count=36}\n```\nsklearn.utils._bunch.Bunch\n```\n:::\n:::\n\n\nIt normally returns a dictionary-like objects (e.g. Bunch) with the following 3 attributes:\n\n-   importances_mean (mean of feature importances)\n-   importances_std (standard deviation of feature importances)\n-   importances (raw permutation/feature importances scores)\n\nFor details on these attributes, this *scikit-learn* [link](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance) will add a bit more explanations.\n\nI decided to only use importances_mean for now.\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\nperm_imp = perm_result.importances_mean\n\n# Confirm it produces an array\ntype(perm_imp)\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```\nnumpy.ndarray\n```\n:::\n:::\n\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\n# Using the function feat_imp_plot() on perm_imp result to show plot\nfeat_imp_plot(perm_imp, X_mp4_df)\n```\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-39-output-1.png){width=732 height=429}\n:::\n:::\n\n\nIt generated a different feature importances ranking (if looking at top 6 features), although somewhat similar to the previous one.\n\n<br>\n\n###### **SHAP approach**\n\nSHAP values [@lundberg2020local2global], [@shapley1953] were used here to provide another way to figure out feature importances. The GitHub repository for this SHAP approach could be accessed [here](https://github.com/shap/shap).\n\nSHAP's TreeExplainer() was based on Tree SHAP algorithms [@lundberg2020local2global], and was used to show and explain feature importances within tree models. It could also be extended to boosted tree models such as LightGBM and XGBoost and also other tree models (as explained by the GitHub repository README.md and its documentation link provided). It was also a model-agnostic method, which could be quite handy.\n\n[*Other reference*](https://mljar.com/blog/feature-importance-in-random-forest/)\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nshap_explainer = shap.TreeExplainer(rfreg)\n\n# X_test needs to be a dataframe (not numpy array)\n# otherwise feature names won't show in plot\nshap_values = shap_explainer.shap_values(X_mp_test_df)\n\n# Horizontal bar plot\nshap.summary_plot(shap_values, X_mp_test_df, plot_type = \"bar\")\n```\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-40-output-1.png){width=756 height=901}\n:::\n:::\n\n\nDot plot version:\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nshap.summary_plot(shap_values, X_mp_test_df)\n```\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-41-output-1.png){width=747 height=900}\n:::\n:::\n\n\nViolin plot:\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\nshap.summary_plot(shap_values, X_mp_test_df, plot_type = \"violin\")\n\n# Alternative plot option: \"layered_violin\"\n```\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-42-output-1.png){width=747 height=900}\n:::\n:::\n\n\n<br>\n\n##### **Hyperparameter tuning**\n\nAn example was shown below on tuning the number of trees (n_estimators) used in the random forest model.\n\n::: {.cell execution_count=42}\n``` {.python .cell-code}\n# Function code adapted with thanks from ML Mastery \n# https://machinelearningmastery.com/random-forest-ensemble-in-python/\n\n# ---Evaluate a list of models with different number of trees---\n\n# Define dataset by using the same training dataset as above\nX, y = X_mp4, y_mp4\n\n# Define function to generate a list of models with different no. of trees\ndef models():\n    # Create empty dictionary (key, value pairs) for models\n    models = dict()\n    # Test different number of trees to evaluate\n    no_trees = [50, 100, 250, 500, 1000]\n    for n in no_trees:\n        models[str(n)] = RandomForestRegressor(n_estimators=n)\n    return models\n\n\n# Define function to evaluate a single model using cross-validation\ndef evaluate(model, X, y):\n\n    # RepeatedStratifiedKFold usually for binary or multi-class labels \n    # - ref link: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold\n    # so using ReaptedKFold instead\n    cross_val = RepeatedKFold(n_splits=10, n_repeats=15, random_state=1)\n    # Run evaluation process & collect cv scores\n    # Since estimator/model was based on DecisionTreeRegressor, \n    # using neg_mean_squared_error metric\n    # n_jobs = -1 meaning using all processors to run jobs in parallel\n    scores = cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=cross_val, n_jobs=-1)\n    return scores\n\n\n# Evaluate results\n# Run models with different RepeatedKFold & different no. of tress\n# with results shown as diff. trees with calculated mean cv scores & std\n\n# Obtain diff. models with diff. trees via models function\nmodels = models()\n\n# Create empty lists for results & names\nresults, names = list(), list()\n\n# Create a for loop to iterate through the list of diff. models\nfor name, model in models.items():\n    # Run the cross validation scores via evaluate function\n    scores = evaluate(model, X, y)\n    # Collect results\n    results.append(scores)\n    # Collect names (different no. of trees)\n    names.append(name)\n    # Show the average mean squared errors and corresponding standard deviations \n    # for each model with diff. no. of trees\n    print((name, mean(scores), std(scores)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n('50', -1.6737576421901594, 1.6620071909344485)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n('100', -1.6891567692982907, 1.6819446214222007)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n('250', -1.6682202069779464, 1.6114488962657219)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n('500', -1.6624517321785266, 1.629219022414753)\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n('1000', -1.6535917235148399, 1.608024351905257)\n```\n:::\n:::\n\n\nThe negated version of the mean squared error (neg_mean_squared_error) was due to how the scoring parameter source code was written in *scikit-learn*. It was written this way to take into account of both *scoring* and *loss* functions (links provided below for further explanations). All scoring metrics could be accessed [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for *scikit-learn*.\n\nReference links to help with understanding neg_mean_squared_error:\n\n1.  [scikit-learn source code](https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/metrics/\\_scorer.py#L624)\n\n2.  [StackOverflow answer](https://stackoverflow.com/questions/48244219/is-sklearn-metrics-mean-squared-error-the-larger-the-better-negated)\n\nAlso, the random forest algorithm was stochastic in nature, meaning that every time hyperparameter tuning took place, it would generate different scores due to random bootstrap sampling. The best approach to evaluate model performance during the cross-validation process was to use the average outcome from several runs of cross-validations, then fit the hyperparameters on a final model, or getting several final models ready and then obtaining the average from these models instead.\n\nBelow was a version of boxplot plotted using Matplotlib showing the differences in the distributions of the cross validation scores and mean squared errors between different number of trees.\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-44-output-1.png){width=569 height=411}\n:::\n:::\n\n\nTo plot this in Seaborn, I had to prepare the data slightly differently to achieve a different version of the boxplot. Matplotlib was a bit more straightforward to use without these steps.\n\nI also used natural sort to sort numerical values ([GitHub repository](https://github.com/SethMMorton/natsort)). Otherwise, if using sort_values() only, it would only sort the numbers in lexicographical order (i.e. by first digit only), which was not able to show the tree numbers in ascending order.\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\n# Combine results & names lists into dataframe\ncv_results = pd.DataFrame(results, index = [names])\n```\n:::\n\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\n# Reset index and rename the number of trees column\ncv_results = cv_results.reset_index().rename(columns={\"level_0\": \"Number_of_trees\"})\n```\n:::\n\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\n# Melt the dataframe by number of trees column\ncv_results = cv_results.melt(id_vars=\"Number_of_trees\")\n```\n:::\n\n\n::: {.cell execution_count=47}\n``` {.python .cell-code}\n# Sort by the number of trees column\ncv_results = cv_results.sort_values(\n    by=\"Number_of_trees\",\n    key=lambda x: np.argsort(index_natsorted(cv_results[\"Number_of_trees\"]))\n)\n```\n:::\n\n\n::: {.cell execution_count=48}\n``` {.python .cell-code}\n# Seaborn boxplot\nsns.boxplot(cv_results, x=\"Number_of_trees\", y=\"value\", showmeans=True)\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\n<Axes: xlabel='Number_of_trees', ylabel='value'>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](1_random_forest_files/figure-html/cell-49-output-2.png){width=587 height=429}\n:::\n:::\n\n\nThe Seaborn boxplot shown should be very similar to the Matplotlib one.\n\nOther hyperparameters that could be tuned included: \n\n-   tree depths (max_depth)\n\n-   number of samples (max_samples) \n\n-   number of features (max_features) - I didn't use RDKit to generate molecular features for this post (Datamol version was used instead) which would provide around 209 at least (trying to keep the post at a readable length), but I think this might be a better option when doing cross-validations in model evaluations\n\n-   number of nodes (max_leaf_nodes)\n\nI've decided not to code for these other hyperparameters in the cross-validation step due to length of post (the function code used in cross-validation above could be further adapted to cater for other hyperparameters mentioned here), but they should be looked into if doing full-scale and comprehensive ML using the ensemble random forest algorithm.\n\n<br>\n\n##### **Final words**\n\nRandom forest was known to be a black-box ML algorithm [@bruce2020], which was completely different from the white-box ML style revealed in decision tree graphs. Feature importances was therefore crucial to shed some lights and remove some layers of the black-box nature in random forest by showing which features were contributing towards model accuracy by ranking features used to train the model. Cross-validation was also vital to avoid over-fitting (which was more applicable to depth of trees), although in some other cases (e.g. number of trees), it was mentioned that it was unlikely the model would be overfitted. Other options available in *scikit-learn* ensemble methods that I didn't get time to try were using voting classifier/regressor and stacking models to reduce biases in models, which might be very useful in other cases.\n\nFew things I've thought of that I could try to improve what I did here was that I should really look for a different set of testing data, rather than using the max phase splits, which was not that ideal. However, as a lot of us are aware, good drug discovery data are hard to come by (a long-standing and complicated problem), I probably need some luck while looking for a different set of drug discovery data later. Another approach that I could try was that I could use RandomForestClassifier() instead on max phase prediction of these small molecules, rather than making pKi value predictions. This might involve re-labelling the max phases for these compounds into a binary or class labels, then I could use the imbalance-learn package to try and alleviate the problem with imbalanced datasets. Nevertheless, I had some fun working on this post and learnt a lot while doing it, and I hope some of the readers might find this post helpful or informative at least.\n\n<br>\n\n##### **Acknowledgement**\n\nI'd like to thank all the authors, developers and contributors who worked towards all of the open-source packages or libraries used in this post. I'd also like to thank all of the other senior cheminformatics and ML practitioners who were sharing their work and knowledge online. \n\n",
    "supporting": [
      "1_random_forest_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}