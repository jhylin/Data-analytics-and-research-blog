{
  "hash": "781a8bf131c88203dcedcfb631ad55b6",
  "result": {
    "markdown": "---\ntitle: Prevent model overfitting in deep neural networks\ndate: 2025-2-11\nauthor: Jennifer HY Lin\ndraft: false\ncategories:\n  - Notes\n  - Deep learning\nformat: html\nfontsize: 12.5pt\n---\n\n*note: there could be more ways to handle model overfitting, so this note is more like a slow-evolving document over time, and it mainly describes approaches applicable to neural networks (NN) built by using PyTorch but I suspect similar concepts may also apply (to a certain degree) towards other deep learning libraries/frameworks such as TensorFlow or Keras (and likely may apply to other machine learning algorithms too)*\n\n<br>\n\n##### **How to prevent overfitting in neural networks?**\n\nIt appears there are three main approaches used to prevent model overfitting:\n\n1. **Drop out layer**\n\n- based on this (preprint) [paper](https://arxiv.org/abs/1207.0580)\n\n- adding a drop out layer is likely better and more useful for a larger NN, and is probably not great for the tiny two-layer NN that has been used in this [post](https://jhylin.github.io/Data_in_life_blog/posts/22_Simple_dnn_adrs/2_ADR_regressor.html)\n\n- there are 2 types: [`nn.Dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) with a [code example](https://machinelearningmastery.com/using-dropout-regularization-in-pytorch-models/) and [`F.dropout()`](https://pytorch.org/docs/stable/generated/torch.nn.functional.dropout.html) (note: F = functional)\n\n- drop out is most effective if used during model training phase\n\n- an [explanation](https://stackoverflow.com/a/53452827/19218378) about differences between these 2 types of drop out, but essentially they're the same but useful to use `F.dropout()` (a functional interface) when there are no parameters (e.g. weights and biases) required and will need to specify if in training or evaluation mode, and use `nn.Dropout()` (a PyTorch module) when parameters are needed with no need to specify the training or evaluation mode since nn.Dropout() will take care of this automatically\n\n<br>\n\n2. **Checkpoints with early stopping**\n\n- based on the concept that PyTorch can retrieve and restore weights or parameters of NN\n- first to save weights or parameters of the model\n\n```{{python}}\ntorch.save(model.state_dict(), model_filename_or_path)\n```\n- then reload the model\n\n```{{python}}\nmodel.load_state_dict(torch.load(model_filename_or_path)) \n```\n- need to set up an early stop threshold and use accuracy (e.g. accuracy as y_predict = model (X_test))\n- one feature is that you can set n_epochs with a very large number as the training loop can be terminated with a code break when there's a threshold set up\n- a [code example](https://machinelearningmastery.com/managing-a-pytorch-training-process-with-checkpoints-and-early-stopping/) which may be useful\n\n<br>\n\n3. **Early stopping in model training loop**\n\nVersion 1:\n\nThe code being used here is inspired and adapted (with thanks) from this [thread](https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch).\n\n```{{python}}\nclass EarlyStopping:\n    def __init__(self, epochs_to_wait = 1, delta = 0):\n        self.epochs_to_wait = epochs_to_wait\n        self.delta = delta\n        self.early_stop = False\n        self.counter = 0\n\n    def __call__(self, test_loss, train_loss):\n        if (test_loss - train_loss) > self.delta:\n            self.counter += 1\n            if self.counter > self.epochs_to_wait:\n                self.early_stop = True\n\nearly_stopper = EarlyStopping(epochs_to_wait = 2, delta = 0)\n\nfor i in range(len(train_epoch_loss)):\n    early_stopper(train_epoch_loss[i], test_epoch_loss[i])\n    print(f\"train loss: {train_epoch_loss[i]} test loss: {test_epoch_loss[i]}\")\n    if early_stopper.early_stop:\n        print(\"Early stop at epoch:\", i)\n        break\n```\n\nThe code output for one of the runs from the [notebook](https://github.com/jhylin/Adverse_drug_reactions/blob/main/3_ADR_reg_early_stop.ipynb):\n\ntrain loss: 1.2966824769973755 test loss: 1.9737834930419922\ntrain loss: 1.293437123298645 test loss: 1.93597412109375\ntrain loss: 1.2902556657791138 test loss: 1.8992326259613037\ntrain loss: 1.2871367931365967 test loss: 1.8635075092315674\ntrain loss: 1.2840790748596191 test loss: 1.828752040863037\ntrain loss: 1.2810810804367065 test loss: 1.7949209213256836\ntrain loss: 1.2781414985656738 test loss: 1.7619731426239014\ntrain loss: 1.275259256362915 test loss: 1.7298691272735596\ntrain loss: 1.2724330425262451 test loss: 1.6985728740692139\ntrain loss: 1.269661545753479 test loss: 1.6680500507354736\ntrain loss: 1.2669436931610107 test loss: 1.6382684707641602\ntrain loss: 1.2642782926559448 test loss: 1.6091980934143066\ntrain loss: 1.2616642713546753 test loss: 1.580810308456421\ntrain loss: 1.2591005563735962 test loss: 1.5532073974609375\ntrain loss: 1.2565860748291016 test loss: 1.5262627601623535\ntrain loss: 1.2541197538375854 test loss: 1.499927043914795\ntrain loss: 1.251700758934021 test loss: 1.474177360534668\ntrain loss: 1.2493280172348022 test loss: 1.4489929676055908\ntrain loss: 1.2470005750656128 test loss: 1.424353837966919\ntrain loss: 1.2447175979614258 test loss: 1.4002411365509033\ntrain loss: 1.2424780130386353 test loss: 1.3766369819641113\ntrain loss: 1.240281105041504 test loss: 1.3535246849060059\ntrain loss: 1.2381259202957153 test loss: 1.3308889865875244\ntrain loss: 1.2360116243362427 test loss: 1.3090163469314575\ntrain loss: 1.2339375019073486 test loss: 1.2876880168914795\ntrain loss: 1.2319027185440063 test loss: 1.2667698860168457\ntrain loss: 1.229906439781189 test loss: 1.246250867843628\ntrain loss: 1.2279479503631592 test loss: 1.226119875907898\ntrain loss: 1.2260264158248901 test loss: 1.2063673734664917\ntrain loss: 1.2241413593292236 test loss: 1.1869832277297974\nEarly stop at epoch: 29\n\nSome comments:\n\n- can alter delta to specify how big the difference is between train and test losses (the gap between them)\n\n- epochs_to_wait can be altered too to specify at least how many epochs need to pass before using early stopping\n\n- due to the small-sized dataset being used here, the delta needs to be at 0 (since the losses are very small as well...), otherwise there won't be an early stop at all and the training epochs will just keep rolling...\n\n- other dataset should bring more interesting results!\n\n- again butina split with shuffling will alter results on each refreshed run (only specific to the notebook I'm using, and this shouldn't be an issue if the data produced in the end are of fixed or set values only)\n\n<br>\n\nVersion 2:\n\nThe code below may need more tweaking. This one focusses more on the test loss trend rather than the gap between train loss and test loss (and to be honest, I somehow understand version 1 better than this one at the moment...)\n\n```{{python}}\nclass Early_stopping:\n\n    ## earlier version:\n    def __init__(self, epochs_to_wait = 5, delta = 0):\n        self.epochs_to_wait = epochs_to_wait\n        self.delta = delta\n        self.min_test_loss = np.inf\n        self.counter = 0\n    \n    def early_stop(self, test_loss):\n        if test_loss < self.min_test_loss: \n            self.min_test_loss = test_loss\n            self.counter = 0\n        elif test_loss > (self.min_test_loss + self.delta):\n            self.counter += 1\n            if self.counter >= self.epochs_to_wait:\n                return True\n        return False\n\n    ## alternative version: \n    # def __init__(self, epochs_to_wait = 1, delta = 0):\n    #     self.epochs_to_wait = epochs_to_wait\n    #     self.delta = delta\n    #     self.min_test_loss = 0\n    #     self.counter = 0\n    #     self.early_stop = False\n\n    # def __call__(self, test_loss):\n    #     if self.min_test_loss == None:\n    #         self.min_test_loss = test_loss\n    #     elif test_loss < self.min_test_loss:\n    #         self.min_test_loss = test_loss\n    #         self.counter = 0    # reset counter to zero if test loss improves\n    #     elif test_loss > (self.min_test_loss + self.delta):\n    #         self.counter += 1\n    #         #print(f\"Early stopping counter {self.counter} of {self.epochs_to_wait}\")\n    #         if self.counter >= self.epochs_to_wait:\n    #             #print(\"Early stopping\")\n    #             self.early_stop = True\n\nearly_stopper = Early_stopping()\n\nfor i in range(len(test_epoch_loss)):\n    #early_stopper(test_epoch_loss[i])\n    print(f\"train loss: {train_epoch_loss[i]} test loss: {test_epoch_loss[i]}\")\n    if early_stopper.early_stop:\n        print(f\"early stop at epoch: {i}\")\n        break\n```\n\nThe code output for one of the runs in the [notebook](https://github.com/jhylin/Adverse_drug_reactions/blob/main/3_ADR_reg_early_stop.ipynb) (note: epoch 0 is first epoch):\n\ntrain loss: 0.4035276174545288 test loss: 0.919343888759613\nearly stop at epoch: 0\n\nIt is also possible to mix checkpoints along with early stoppings in the model training loops (set up your own functions/classes according to needs), a [code example](https://machinelearningmastery.com/managing-a-pytorch-training-process-with-checkpoints-and-early-stopping/) will be the section on \"Checkpointing with Early Stopping\" from the same link as provided above.\n\n<br>\n\n##### **Other related readings that might be of interest**\n\n* Unsure how useful this may be, but I happen to come across a [preprint paper](https://arxiv.org/pdf/2501.19195) while working on this note and it talks about how early stopping on validation loss is likely going to lead to problems with calibration and refinement errors (components of cross-entropy), and what they're using to overcome this (its [GitHub repo](https://github.com/dholzmueller/probmetrics))\n\n* and I'm sure there will be others in the literatures\n\n",
    "supporting": [
      "4_Prevent_overfit_note_files"
    ],
    "filters": [],
    "includes": {}
  }
}